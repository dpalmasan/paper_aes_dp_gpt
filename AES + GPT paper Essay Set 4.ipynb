{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f104690",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import syllables\n",
    "from collections import defaultdict\n",
    "from pqdm.processes import pqdm\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from textblob import TextBlob\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import spacy\n",
    "from numpy.linalg import norm\n",
    "from tqdm import tqdm\n",
    "\n",
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "\n",
    "client = OpenAI(api_key=os.environ[\"OPEN_AI_API_KEY\"])\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "ESSAY_SET = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415ccb28",
   "metadata": {},
   "source": [
    "# Define QWK Metric functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "782592ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix(rater_a, rater_b, min_rating=None, max_rating=None):\n",
    "    \"\"\"\n",
    "    Returns the confusion matrix between rater's ratings\n",
    "    \"\"\"\n",
    "    assert(len(rater_a) == len(rater_b))\n",
    "    if min_rating is None:\n",
    "        min_rating = min(rater_a + rater_b)\n",
    "    if max_rating is None:\n",
    "        max_rating = max(rater_a + rater_b)\n",
    "    num_ratings = int(max_rating - min_rating + 1)\n",
    "    conf_mat = [[0 for i in range(num_ratings)]\n",
    "                for j in range(num_ratings)]\n",
    "    for a, b in zip(rater_a, rater_b):\n",
    "        conf_mat[a - min_rating][b - min_rating] += 1\n",
    "    return conf_mat\n",
    "\n",
    "\n",
    "def histogram(ratings, min_rating=None, max_rating=None):\n",
    "    \"\"\"\n",
    "    Returns the counts of each type of rating that a rater made\n",
    "    \"\"\"\n",
    "    if min_rating is None:\n",
    "        min_rating = min(ratings)\n",
    "    if max_rating is None:\n",
    "        max_rating = max(ratings)\n",
    "    num_ratings = int(max_rating - min_rating + 1)\n",
    "    hist_ratings = [0 for x in range(num_ratings)]\n",
    "    for r in ratings:\n",
    "        hist_ratings[r - min_rating] += 1\n",
    "    return hist_ratings\n",
    "\n",
    "\n",
    "def quadratic_weighted_kappa(rater_a, rater_b, min_rating=None, max_rating=None):\n",
    "    \"\"\"\n",
    "    Calculates the quadratic weighted kappa\n",
    "    quadratic_weighted_kappa calculates the quadratic weighted kappa\n",
    "    value, which is a measure of inter-rater agreement between two raters\n",
    "    that provide discrete numeric ratings.  Potential values range from -1\n",
    "    (representing complete disagreement) to 1 (representing complete\n",
    "    agreement).  A kappa value of 0 is expected if all agreement is due to\n",
    "    chance.\n",
    "\n",
    "    quadratic_weighted_kappa(rater_a, rater_b), where rater_a and rater_b\n",
    "    each correspond to a list of integer ratings.  These lists must have the\n",
    "    same length.\n",
    "\n",
    "    The ratings should be integers, and it is assumed that they contain\n",
    "    the complete range of possible ratings.\n",
    "\n",
    "    quadratic_weighted_kappa(X, min_rating, max_rating), where min_rating\n",
    "    is the minimum possible rating, and max_rating is the maximum possible\n",
    "    rating\n",
    "    \"\"\"\n",
    "    rater_a = np.array(rater_a, dtype=int)\n",
    "    rater_b = np.array(rater_b, dtype=int)\n",
    "    assert(len(rater_a) == len(rater_b))\n",
    "    if min_rating is None:\n",
    "        min_rating = min(min(rater_a), min(rater_b))\n",
    "    if max_rating is None:\n",
    "        max_rating = max(max(rater_a), max(rater_b))\n",
    "    conf_mat = confusion_matrix(rater_a, rater_b,\n",
    "                                min_rating, max_rating)\n",
    "    num_ratings = len(conf_mat)\n",
    "    num_scored_items = float(len(rater_a))\n",
    "\n",
    "    hist_rater_a = histogram(rater_a, min_rating, max_rating)\n",
    "    hist_rater_b = histogram(rater_b, min_rating, max_rating)\n",
    "\n",
    "    numerator = 0.0\n",
    "    denominator = 0.0\n",
    "\n",
    "    for i in range(num_ratings):\n",
    "        for j in range(num_ratings):\n",
    "            expected_count = (hist_rater_a[i] * hist_rater_b[j]\n",
    "                              / num_scored_items)\n",
    "            d = pow(i - j, 2.0) / pow(num_ratings - 1, 2.0)\n",
    "            numerator += d * conf_mat[i][j] / num_scored_items\n",
    "            denominator += d * expected_count / num_scored_items\n",
    "\n",
    "    return 1.0 - numerator / denominator\n",
    "\n",
    "\n",
    "def linear_weighted_kappa(rater_a, rater_b, min_rating=None, max_rating=None):\n",
    "    \"\"\"\n",
    "    Calculates the linear weighted kappa\n",
    "    linear_weighted_kappa calculates the linear weighted kappa\n",
    "    value, which is a measure of inter-rater agreement between two raters\n",
    "    that provide discrete numeric ratings.  Potential values range from -1\n",
    "    (representing complete disagreement) to 1 (representing complete\n",
    "    agreement).  A kappa value of 0 is expected if all agreement is due to\n",
    "    chance.\n",
    "\n",
    "    linear_weighted_kappa(rater_a, rater_b), where rater_a and rater_b\n",
    "    each correspond to a list of integer ratings.  These lists must have the\n",
    "    same length.\n",
    "\n",
    "    The ratings should be integers, and it is assumed that they contain\n",
    "    the complete range of possible ratings.\n",
    "\n",
    "    linear_weighted_kappa(X, min_rating, max_rating), where min_rating\n",
    "    is the minimum possible rating, and max_rating is the maximum possible\n",
    "    rating\n",
    "    \"\"\"\n",
    "    assert(len(rater_a) == len(rater_b))\n",
    "    if min_rating is None:\n",
    "        min_rating = min(rater_a + rater_b)\n",
    "    if max_rating is None:\n",
    "        max_rating = max(rater_a + rater_b)\n",
    "    conf_mat = confusion_matrix(rater_a, rater_b,\n",
    "                                min_rating, max_rating)\n",
    "    num_ratings = len(conf_mat)\n",
    "    num_scored_items = float(len(rater_a))\n",
    "\n",
    "    hist_rater_a = histogram(rater_a, min_rating, max_rating)\n",
    "    hist_rater_b = histogram(rater_b, min_rating, max_rating)\n",
    "\n",
    "    numerator = 0.0\n",
    "    denominator = 0.0\n",
    "\n",
    "    for i in range(num_ratings):\n",
    "        for j in range(num_ratings):\n",
    "            expected_count = (hist_rater_a[i] * hist_rater_b[j]\n",
    "                              / num_scored_items)\n",
    "            d = abs(i - j) / float(num_ratings - 1)\n",
    "            numerator += d * conf_mat[i][j] / num_scored_items\n",
    "            denominator += d * expected_count / num_scored_items\n",
    "\n",
    "    return 1.0 - numerator / denominator\n",
    "\n",
    "\n",
    "def kappa(rater_a, rater_b, min_rating=None, max_rating=None):\n",
    "    \"\"\"\n",
    "    Calculates the kappa\n",
    "    kappa calculates the kappa\n",
    "    value, which is a measure of inter-rater agreement between two raters\n",
    "    that provide discrete numeric ratings.  Potential values range from -1\n",
    "    (representing complete disagreement) to 1 (representing complete\n",
    "    agreement).  A kappa value of 0 is expected if all agreement is due to\n",
    "    chance.\n",
    "\n",
    "    kappa(rater_a, rater_b), where rater_a and rater_b\n",
    "    each correspond to a list of integer ratings.  These lists must have the\n",
    "    same length.\n",
    "\n",
    "    The ratings should be integers, and it is assumed that they contain\n",
    "    the complete range of possible ratings.\n",
    "\n",
    "    kappa(X, min_rating, max_rating), where min_rating\n",
    "    is the minimum possible rating, and max_rating is the maximum possible\n",
    "    rating\n",
    "    \"\"\"\n",
    "    assert(len(rater_a) == len(rater_b))\n",
    "    if min_rating is None:\n",
    "        min_rating = min(rater_a + rater_b)\n",
    "    if max_rating is None:\n",
    "        max_rating = max(rater_a + rater_b)\n",
    "    conf_mat = confusion_matrix(rater_a, rater_b,\n",
    "                                min_rating, max_rating)\n",
    "    num_ratings = len(conf_mat)\n",
    "    num_scored_items = float(len(rater_a))\n",
    "\n",
    "    hist_rater_a = histogram(rater_a, min_rating, max_rating)\n",
    "    hist_rater_b = histogram(rater_b, min_rating, max_rating)\n",
    "\n",
    "    numerator = 0.0\n",
    "    denominator = 0.0\n",
    "\n",
    "    for i in range(num_ratings):\n",
    "        for j in range(num_ratings):\n",
    "            expected_count = (hist_rater_a[i] * hist_rater_b[j]\n",
    "                              / num_scored_items)\n",
    "            if i == j:\n",
    "                d = 0.0\n",
    "            else:\n",
    "                d = 1.0\n",
    "            numerator += d * conf_mat[i][j] / num_scored_items\n",
    "            denominator += d * expected_count / num_scored_items\n",
    "\n",
    "    return 1.0 - numerator / denominator\n",
    "\n",
    "\n",
    "def mean_quadratic_weighted_kappa(kappas, weights=None):\n",
    "    \"\"\"\n",
    "    Calculates the mean of the quadratic\n",
    "    weighted kappas after applying Fisher's r-to-z transform, which is\n",
    "    approximately a variance-stabilizing transformation.  This\n",
    "    transformation is undefined if one of the kappas is 1.0, so all kappa\n",
    "    values are capped in the range (-0.999, 0.999).  The reverse\n",
    "    transformation is then applied before returning the result.\n",
    "\n",
    "    mean_quadratic_weighted_kappa(kappas), where kappas is a vector of\n",
    "    kappa values\n",
    "\n",
    "    mean_quadratic_weighted_kappa(kappas, weights), where weights is a vector\n",
    "    of weights that is the same size as kappas.  Weights are applied in the\n",
    "    z-space\n",
    "    \"\"\"\n",
    "    kappas = np.array(kappas, dtype=float)\n",
    "    if weights is None:\n",
    "        weights = np.ones(np.shape(kappas))\n",
    "    else:\n",
    "        weights = weights / np.mean(weights)\n",
    "\n",
    "    # ensure that kappas are in the range [-.999, .999]\n",
    "    kappas = np.array([min(x, .999) for x in kappas])\n",
    "    kappas = np.array([max(x, -.999) for x in kappas])\n",
    "\n",
    "    z = 0.5 * np.log((1 + kappas) / (1 - kappas)) * weights\n",
    "    z = np.mean(z)\n",
    "    return (np.exp(2 * z) - 1) / (np.exp(2 * z) + 1)\n",
    "\n",
    "\n",
    "def weighted_mean_quadratic_weighted_kappa(solution, submission):\n",
    "    predicted_score = submission[submission.columns[-1]].copy()\n",
    "    predicted_score.name = \"predicted_score\"\n",
    "    if predicted_score.index[0] == 0:\n",
    "        predicted_score = predicted_score[:len(solution)]\n",
    "        predicted_score.index = solution.index\n",
    "    combined = solution.join(predicted_score, how=\"left\")\n",
    "    groups = combined.groupby(by=\"essay_set\")\n",
    "    kappas = [quadratic_weighted_kappa(group[1][\"essay_score\"], group[1][\"predicted_score\"]) for group in groups]\n",
    "    weights = [group[1][\"essay_weight\"].irow(0) for group in groups]\n",
    "    return mean_quadratic_weighted_kappa(kappas, weights=weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d4bafaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "UNIVERSAL_NOUN_TAGS = set([u\"NOUN\", u\"PRON\", u\"PROPN\"])\n",
    "\n",
    "ordered_transitions = [\n",
    "    u\"SS\",\n",
    "    u\"SO\",\n",
    "    u\"SX\",\n",
    "    u\"S-\",\n",
    "    u\"OS\",\n",
    "    u\"OO\",\n",
    "    u\"OX\",\n",
    "    u\"O-\",\n",
    "    u\"XS\",\n",
    "    u\"XO\",\n",
    "    u\"XX\",\n",
    "    u\"X-\",\n",
    "    u\"-S\",\n",
    "    u\"-O\",\n",
    "    u\"-X\",\n",
    "    u\"--\",\n",
    "]\n",
    "\n",
    "\n",
    "def dependency_mapping(dep: str) -> str:\n",
    "    \"\"\"Map dependency tag to entity grid tag.\n",
    "\n",
    "    We consider the notation provided in :cite:`barzilay2008modeling`:\n",
    "\n",
    "    +-----------+-----------------------------------+\n",
    "    | EGrid Tag | Dependency Tag                    |\n",
    "    +===========+===================================+\n",
    "    | S         | nsub, csubj, csubjpass, dsubjpass |\n",
    "    +-----------+-----------------------------------+\n",
    "    | O         | iobj, obj, pobj, dobj             |\n",
    "    +-----------+-----------------------------------+\n",
    "    | X         | For any other dependency tag      |\n",
    "    +-----------+-----------------------------------+\n",
    "\n",
    "    :param dep: Dependency tag\n",
    "    :type dep: string\n",
    "    :return: EGrid tag\n",
    "    :rtype: string\n",
    "    \"\"\"\n",
    "    if dep in {u\"nsubj\", u\"csubj\", u\"csubjpass\", u\"dsubjpass\"}:\n",
    "        return u\"S\"\n",
    "    if dep in {u\"iobj\", u\"obj\", u\"pobj\", u\"dobj\"}:\n",
    "        return u\"O\"\n",
    "\n",
    "    return \"X\"\n",
    "\n",
    "\n",
    "class EntityGrid(object):\n",
    "    \"\"\"Entity grid class.\n",
    "\n",
    "    Class Entity Grid, creates an entity grid from a doc, which is output of\n",
    "    applying spacy.nlp(text) to a text. Thus, this class depends on spacy\n",
    "    module. It only supports 2-transitions entity grid.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, doc):\n",
    "        \"\"\"Construct EntityGrid object.\"\"\"\n",
    "        # Initialization\n",
    "        entity_map = dict()\n",
    "        entity_grid = dict()\n",
    "        i = 1\n",
    "        entity_map[\"s%d\" % i] = []\n",
    "        entity_features = {\n",
    "            u\"SS\": 0,\n",
    "            u\"SO\": 0,\n",
    "            u\"SX\": 0,\n",
    "            u\"S-\": 0,\n",
    "            u\"OS\": 0,\n",
    "            u\"OO\": 0,\n",
    "            u\"OX\": 0,\n",
    "            u\"O-\": 0,\n",
    "            u\"XS\": 0,\n",
    "            u\"XO\": 0,\n",
    "            u\"XX\": 0,\n",
    "            u\"X-\": 0,\n",
    "            u\"-S\": 0,\n",
    "            u\"-O\": 0,\n",
    "            u\"-X\": 0,\n",
    "            u\"--\": 0,\n",
    "        }\n",
    "\n",
    "\n",
    "        n_sent = len(list(doc.sents))\n",
    "\n",
    "        # To get coherence measurements we need at least 2 sentences\n",
    "        if n_sent < 2:\n",
    "            raise RuntimeError(\n",
    "                \"Entity grid needs at least two sentences, found: {}\".format(\n",
    "                    n_sent\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # For each sentence, get dependencies and its grammatical role\n",
    "        for sent in doc.sents:\n",
    "            for token in sent:\n",
    "                if token.pos_ in UNIVERSAL_NOUN_TAGS:\n",
    "                    entity_map[\"s%d\" % i].append(\n",
    "                        (token.text.upper(), token.dep_)\n",
    "                    )\n",
    "                    if token.text.upper() not in entity_grid:\n",
    "                        entity_grid[token.text.upper()] = [u\"-\"] * n_sent\n",
    "            i += 1\n",
    "            entity_map[\"s%d\" % i] = []\n",
    "\n",
    "        # Last iteration will create an extra element, so I remove it.\n",
    "        entity_map.pop(\"s%d\" % i)\n",
    "\n",
    "        # Fill entity grid\n",
    "        for i in range(n_sent):\n",
    "            sentence = \"s%d\" % (i + 1)\n",
    "            for entity, dep in entity_map[sentence]:\n",
    "                if entity_grid[entity][i] == u\"-\":\n",
    "                    entity_grid[entity][i] = dependency_mapping(dep)\n",
    "                elif dependency_mapping(dep) == u\"S\":\n",
    "                    entity_grid[entity][i] = dependency_mapping(dep)\n",
    "                elif (\n",
    "                    dependency_mapping(dep) == u\"O\"\n",
    "                    and entity_grid[entity][i] == u\"X\"\n",
    "                ):\n",
    "                    entity_grid[entity][i] = dependency_mapping(dep)\n",
    "\n",
    "        # Compute feature vector, we consider transitions of length 2\n",
    "        total_transitions = (n_sent - 1) * len(entity_grid.keys())\n",
    "\n",
    "        for entity in entity_grid:\n",
    "            for i in range(n_sent - 1):\n",
    "                # Transition type found (e.g. S-)\n",
    "                transition = (\n",
    "                    entity_grid[entity][i] + entity_grid[entity][i + 1]\n",
    "                )\n",
    "\n",
    "                # Adding 1 to transition count\n",
    "                entity_features[transition] += 1\n",
    "\n",
    "        for prob in entity_features:\n",
    "            if total_transitions != 0:\n",
    "                entity_features[prob] /= float(total_transitions)\n",
    "            else:\n",
    "                entity_features[prob] = 0.0\n",
    "\n",
    "        self.__grid = entity_grid\n",
    "        self.__n_sent = n_sent\n",
    "        self.__prob = entity_features\n",
    "\n",
    "    def get_ss_transitions(self) -> float:\n",
    "        \"\"\"Get SS transitions.\n",
    "\n",
    "        :return: Ratio of transitions\n",
    "        :rtype: float\n",
    "        \"\"\"\n",
    "        return self.__prob[u\"SS\"]\n",
    "\n",
    "    def get_so_transitions(self) -> float:\n",
    "        \"\"\"Get SO transitions.\n",
    "\n",
    "        :return: Ratio of transitions\n",
    "        :rtype: float\n",
    "        \"\"\"\n",
    "        return self.__prob[u\"SO\"]\n",
    "\n",
    "    def get_sx_transitions(self) -> float:\n",
    "        \"\"\"Get SX transitions.\n",
    "\n",
    "        :return: Ratio of transitions\n",
    "        :rtype: float\n",
    "        \"\"\"\n",
    "        return self.__prob[u\"SX\"]\n",
    "\n",
    "    def get_sn_transitions(self) -> float:\n",
    "        \"\"\"Get S- transitions.\n",
    "\n",
    "        :return: Ratio of transitions\n",
    "        :rtype: float\n",
    "        \"\"\"\n",
    "        return self.__prob[u\"S-\"]\n",
    "\n",
    "    def get_os_transitions(self) -> float:\n",
    "        \"\"\"Get OS transitions.\n",
    "\n",
    "        :return: Ratio of transitions\n",
    "        :rtype: float\n",
    "        \"\"\"\n",
    "        return self.__prob[u\"OS\"]\n",
    "\n",
    "    def get_oo_transitions(self) -> float:\n",
    "        \"\"\"Get OO transitions.\n",
    "\n",
    "        :return: Ratio of transitions\n",
    "        :rtype: float\n",
    "        \"\"\"\n",
    "        return self.__prob[u\"OO\"]\n",
    "\n",
    "    def get_ox_transitions(self) -> float:\n",
    "        \"\"\"Get OX transitions.\n",
    "\n",
    "        :return: Ratio of transitions\n",
    "        :rtype: float\n",
    "        \"\"\"\n",
    "        return self.__prob[u\"OX\"]\n",
    "\n",
    "    def get_on_transitions(self) -> float:\n",
    "        \"\"\"Get O- transitions.\n",
    "\n",
    "        :return: Ratio of transitions\n",
    "        :rtype: float\n",
    "        \"\"\"\n",
    "        return self.__prob[u\"O-\"]\n",
    "\n",
    "    def get_xs_transitions(self) -> float:\n",
    "        \"\"\"Get XS transitions.\n",
    "\n",
    "        :return: Ratio of transitions\n",
    "        :rtype: float\n",
    "        \"\"\"\n",
    "        return self.__prob[u\"XS\"]\n",
    "\n",
    "    def get_xo_transitions(self) -> float:\n",
    "        \"\"\"Get XO transitions.\n",
    "\n",
    "        :return: Ratio of transitions\n",
    "        :rtype: float\n",
    "        \"\"\"\n",
    "        return self.__prob[u\"XO\"]\n",
    "\n",
    "    def get_xx_transitions(self) -> float:\n",
    "        \"\"\"Get XX transitions.\n",
    "\n",
    "        :return: Ratio of transitions\n",
    "        :rtype: float\n",
    "        \"\"\"\n",
    "        return self.__prob[u\"XX\"]\n",
    "\n",
    "    def get_xn_transitions(self) -> float:\n",
    "        \"\"\"Get X- transitions.\n",
    "\n",
    "        :return: Ratio of transitions\n",
    "        :rtype: float\n",
    "        \"\"\"\n",
    "        return self.__prob[u\"X-\"]\n",
    "\n",
    "    def get_ns_transitions(self) -> float:\n",
    "        \"\"\"Get -S transitions.\n",
    "\n",
    "        :return: Ratio of transitions\n",
    "        :rtype: float\n",
    "        \"\"\"\n",
    "        return self.__prob[u\"-S\"]\n",
    "\n",
    "    def get_no_transitions(self) -> float:\n",
    "        \"\"\"Get -O transitions.\n",
    "\n",
    "        :return: Ratio of transitions\n",
    "        :rtype: float\n",
    "        \"\"\"\n",
    "        return self.__prob[u\"-O\"]\n",
    "\n",
    "    def get_nx_transitions(self) -> float:\n",
    "        \"\"\"Get -X transitions.\n",
    "\n",
    "        :return: Ratio of transitions\n",
    "        :rtype: float\n",
    "        \"\"\"\n",
    "        return self.__prob[u\"-X\"]\n",
    "\n",
    "    def get_nn_transitions(self) -> float:\n",
    "        \"\"\"Get -- transitions.\n",
    "\n",
    "        :return: Ratio of transitions\n",
    "        :rtype: float\n",
    "        \"\"\"\n",
    "        return self.__prob[u\"--\"]\n",
    "\n",
    "    def get_egrid(self) -> dict:\n",
    "        \"\"\"Return obtained entity grid (for debugging purposes).\n",
    "\n",
    "        :return: entity grid represented as a dict\n",
    "        :rtype: dict\n",
    "        \"\"\"\n",
    "        return self.__grid\n",
    "\n",
    "    def get_sentence_count(self) -> int:\n",
    "        \"\"\"Return sentence count obtained while processing.\n",
    "\n",
    "        :return: Number of sentences\n",
    "        :rtype: int\n",
    "        \"\"\"\n",
    "        return self.__n_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "10224389",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighting_syntactic_role(entity_role: str) -> int:\n",
    "    \"\"\"Return weight given an entity grammatical role.\n",
    "\n",
    "    Weighting scheme for syntactic role of an entity. This uses the heuristic\n",
    "    from :cite:`guinaudeau2013graph`, which is:\n",
    "\n",
    "    +-----------+--------+\n",
    "    | EGrid Tag | Weight |\n",
    "    +===========+========+\n",
    "    | S         | 3      |\n",
    "    +-----------+--------+\n",
    "    | O         | 2      |\n",
    "    +-----------+--------+\n",
    "    | X         | 1      |\n",
    "    +-----------+--------+\n",
    "    | dash      | 0      |\n",
    "    +-----------+--------+\n",
    "\n",
    "    :param entity_role: Entity grammatical role (S, O, X, -)\n",
    "    :type entity_role: string\n",
    "    :return: Role weight\n",
    "    :rtype: int\n",
    "    \"\"\"\n",
    "    if entity_role == u\"S\":\n",
    "        return 3\n",
    "    elif entity_role == u\"O\":\n",
    "        return 2\n",
    "    elif entity_role == u\"X\":\n",
    "        return 1\n",
    "\n",
    "    return 0\n",
    "\n",
    "\n",
    "def get_local_coherence(egrid: EntityGrid) -> [float, float, float, float]:\n",
    "    \"\"\"Get local coherence from entity grid.\n",
    "\n",
    "    This method gets the coherence value using all the approaches described\n",
    "    in :cite:`guinaudeau2013graph`. This include:\n",
    "\n",
    "    * local_coherence_PU\n",
    "    * local_coherence_PW\n",
    "    * local_coherence_PACC\n",
    "    * local_coherence_PU_dist\n",
    "    * local_coherence_PW_dist\n",
    "    * local_coherence_PACC_dist\n",
    "\n",
    "    :param egrid: An EntityGrid object.\n",
    "    :type egrid: EntityGrid\n",
    "    :return: Local coherence based on different heuristics\n",
    "    :rtype: tuple of floats\n",
    "    \"\"\"\n",
    "    n_sent = egrid.get_sentence_count()\n",
    "\n",
    "    # If entity grid is not valid\n",
    "    if n_sent < 2:\n",
    "        return (0.0, 0.0, 0.0, 0.0, 0.0, 0.0)\n",
    "\n",
    "    PW = [[0] * n_sent for i in range(n_sent)]\n",
    "\n",
    "    # Weight Matrix for PACC, syntactic information is accounted for by\n",
    "    # integrating the edges of the bipartite graph\n",
    "    W = [[0] * n_sent for i in range(n_sent)]\n",
    "\n",
    "    grid = egrid.get_egrid()\n",
    "    for entity in grid:\n",
    "        for i in range(n_sent):\n",
    "            for j in range(i + 1, n_sent):\n",
    "                if grid[entity][i] != u\"-\" and grid[entity][j] != u\"-\":\n",
    "                    PW[i][j] += 1\n",
    "                    W[i][j] += weighting_syntactic_role(\n",
    "                        grid[entity][i]\n",
    "                    ) * weighting_syntactic_role(grid[entity][j])\n",
    "\n",
    "    PU = [list(map(lambda x: x != 0, PWi)) for PWi in PW]\n",
    "\n",
    "    local_coherence_PU = 0.0\n",
    "    local_coherence_PW = 0.0\n",
    "    local_coherence_PACC = 0.0\n",
    "    for i in range(n_sent):\n",
    "        local_coherence_PW += sum(PW[i])\n",
    "        local_coherence_PU += sum(PU[i])\n",
    "        local_coherence_PACC += sum(W[i])\n",
    "\n",
    "    local_coherence_PW /= n_sent\n",
    "    local_coherence_PU /= n_sent\n",
    "    local_coherence_PACC /= n_sent\n",
    "\n",
    "    # Weighting projection graphs\n",
    "    PU_weighted = list(PU)\n",
    "    PW_weighted = list(PW)\n",
    "    PACC_weighted = list(W)\n",
    "    for i in range(n_sent):\n",
    "        for j in range(i + 1, n_sent):\n",
    "            PU_weighted[i][j] = PU[i][j] / float(j - i)\n",
    "            PW_weighted[i][j] = PW[i][j] / float(j - i)\n",
    "            PACC_weighted[i][j] = W[i][j] / float(j - i)\n",
    "\n",
    "    local_coherence_PU_dist = 0.0\n",
    "    local_coherence_PW_dist = 0.0\n",
    "    local_coherence_PACC_dist = 0.0\n",
    "    for i in range(n_sent):\n",
    "        local_coherence_PW_dist += sum(PW_weighted[i])\n",
    "        local_coherence_PU_dist += sum(PU_weighted[i])\n",
    "        local_coherence_PACC_dist += sum(PACC_weighted[i])\n",
    "\n",
    "    local_coherence_PW_dist /= n_sent\n",
    "    local_coherence_PU_dist /= n_sent\n",
    "    local_coherence_PACC_dist /= n_sent\n",
    "    return (\n",
    "        local_coherence_PU,\n",
    "        local_coherence_PW,\n",
    "        local_coherence_PACC,\n",
    "        local_coherence_PU_dist,\n",
    "        local_coherence_PW_dist,\n",
    "        local_coherence_PACC_dist,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bed78faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\n",
    "    f\"./training_set_rel3.tsv\",\n",
    "    sep=\"\\t\",\n",
    "    encoding=\"ISO-8859-1\",\n",
    "    usecols=[\"essay_id\", \"essay_set\", \"essay\", \"domain1_score\", \"domain2_score\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "af931dec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 311, 3: 253, 2: 570, 1: 636})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "df = df[df.essay_set == ESSAY_SET]\n",
    "Counter(df.domain1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3875b716",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_rating, max_rating = int(df.domain1_score.min()), int(df.domain1_score.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b69e1cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ft = pd.DataFrame({\n",
    "    \"prompt\": df.essay.tolist(),\n",
    "    \"completion\": df.domain1_score.tolist()})\n",
    "\n",
    "df_ft.to_json(f\"essay_set{ESSAY_SET}.jsonl\", orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9a7d1079",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>completion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The author concludes the story with this becau...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The narrater has that in with Paragraph becuse...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The author concludes the story with that passa...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The author ended the story with this paragraph...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The author concludes the story with this parag...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              prompt  completion\n",
       "0  The author concludes the story with this becau...           0\n",
       "1  The narrater has that in with Paragraph becuse...           0\n",
       "2  The author concludes the story with that passa...           3\n",
       "3  The author ended the story with this paragraph...           2\n",
       "4  The author concludes the story with this parag...           2"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ft.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e06fdf",
   "metadata": {},
   "source": [
    "# Generate data for Fine Tuning\n",
    "\n",
    "We run the following tool:\n",
    "\n",
    "`openai tools fine_tunes.prepare_data -f essay_set4.jsonl -q`\n",
    "\n",
    "This will generate two data splits for the fine-tuning. One for training and other for validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ddc38e9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FineTuningJob(id='ftjob-AW8uX2evtzbjfcfIzkV1nUrJ', created_at=1721583771, error=Error(code=None, message=None, param=None), fine_tuned_model=None, finished_at=None, hyperparameters=Hyperparameters(n_epochs='auto', batch_size='auto', learning_rate_multiplier='auto'), model='babbage-002', object='fine_tuning.job', organization_id='org-JihYzTh2GjJjoPtZZ0kQdsbr', result_files=[], seed=137058166, status='validating_files', trained_tokens=None, training_file='file-uyQPENGAQX2VoPCNe8m8jsXV', validation_file='file-20SKzaqhTkQm8x6H5bL1OGS1', estimated_finish=None, integrations=[], user_provided_suffix=None)\n"
     ]
    }
   ],
   "source": [
    "train_file = client.files.create(file=open(f\"essay_set{ESSAY_SET}_prepared_train.jsonl\", \"rb\"), purpose=\"fine-tune\")\n",
    "valid_file = client.files.create(file=open(f\"essay_set{ESSAY_SET}_prepared_valid.jsonl\", \"rb\"), purpose=\"fine-tune\")\n",
    "fine_tuning_job = client.fine_tuning.jobs.create(training_file=train_file.id, validation_file=valid_file.id, model=\"babbage-002\")\n",
    "print(fine_tuning_job)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8695dcf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts, scores = (\n",
    "    df.essay.tolist(),\n",
    "    df.domain1_score.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "52762763",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(min_df=10)\n",
    "\n",
    "doc_term = vectorizer.fit_transform(df.essay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d0d9b0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting CVA features https://files.eric.ed.gov/fulltext/ED525309.pdf\n",
    "\n",
    "content_vectors = {}\n",
    "n = df.shape[0]\n",
    "ni = doc_term.getnnz(axis=0)\n",
    "for i in range(min_rating, max_rating + 1):\n",
    "    essay_score_cat = df[df.domain1_score == i]\n",
    "    freqs = vectorizer.transform(essay_score_cat.essay)\n",
    "    \n",
    "    # Get frequencies score categories\n",
    "    fis = freqs.sum(axis=0)\n",
    "    fis = np.asarray(fis).reshape(-1)\n",
    "    try:\n",
    "        max_fs = freqs.max()\n",
    "    except:\n",
    "        print(i)\n",
    "        raise\n",
    "    content_vectors[i] = fis/max_fs * np.log(n / ni)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3b724582",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(844,)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content_vectors[3].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3287935e",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_text = \"\"\"\n",
    "It was like walking into another world. A hot, moist world exploding with\n",
    "greenery. Huge flat leaves, delicate wisps of tendrils, ferns and fronds and vines of\n",
    "all shades and shapes grew in seemingly random profusion.\n",
    "“Over there, in the corner, the hibiscus. Is that what you mean?” The florist\n",
    "pointed at a leafy potted plant by the corner.\n",
    "There, in a shaft of the wan afternoon sunlight, was a single bloodred blossom,\n",
    "its five petals splayed back to reveal a long stamen tipped with yellow pollen.\n",
    "Saeng felt a shock of recognition so intense, it was almost visceral.\n",
    "“Saebba,” Saeng whispered.\n",
    "A saebba hedge, tall and lush, had surrounded their garden, its lush green leaves\n",
    "dotted with vermilion flowers. And sometimes after a monsoon rain, a blossom or\n",
    "two would have blown into the well, so that when she drew up the well water, she\n",
    "would find a red blossom floating in the bucket.\n",
    "Slowly, Saeng walked down the narrow aisle toward the hibiscus. Orchids,\n",
    "lanna bushes, oleanders, elephant ear begonias, and bougainvillea vines\n",
    "surrounded her. Plants that she had not even realized she had known but had\n",
    "forgotten drew her back into her childhood world.\n",
    "When she got to the hibiscus, she reached out and touched a petal gently. It felt\n",
    "smooth and cool, with a hint of velvet toward the center—just as she had known it\n",
    "would feel.\n",
    "9\n",
    "THE WINTER HIBISCUS\n",
    "And beside it was yet another old friend, a small shrub with waxy leaves and\n",
    "dainty flowers with purplish petals and white centers. “Madagascar periwinkle,”\n",
    "its tag announced. How strange to see it in a pot, Saeng thought. Back home it just\n",
    "grew wild, jutting out from the cracks in brick walls or between tiled roofs. There\n",
    "had been a patch of it by the little spirit house where she used to help her mother\n",
    "light the incense and candles to the spirit who guarded their home and their family.\n",
    "Sometimes she would casually pick a flower or two to leave on the offerings of\n",
    "fruit and rice left at the altar.\n",
    "And that rich, sweet scent—that was familiar, too. Saeng scanned the greenery\n",
    "around her and found a tall, gangly plant with exquisite little white blossoms on it.\n",
    "“Dok Malik,” she said, savoring the feel of the word on her tongue, even as she\n",
    "silently noted the English name on its tag, “jasmine.”\n",
    "One of the blossoms had fallen off, and carefully Saeng picked it up and smelled\n",
    "it. She closed her eyes and breathed in, deeply. The familiar fragrance filled her\n",
    "lungs, and Saeng could almost feel the light strands of her grandmother’s long gray\n",
    "hair, freshly washed, as she combed it out with the fine-toothed buffalo-horn\n",
    "comb. And when the sun had dried it, Saeng would help the gnarled old fingers\n",
    "knot the hair into a bun, then slip a dok Malik bud into it.\n",
    "Saeng looked at the white bud in her hand now, small and fragile. Gently, she\n",
    "closed her palm around it and held it tight. That, at least, she could hold on to. But\n",
    "where was the finetoothed comb? The hibiscus hedge? The well? Her gentle\n",
    "grandmother?\n",
    "A wave of loss so deep and strong that it stung Saeng’s eyes now swept over her.\n",
    "A blink, a channel switch, a boat ride in the night, and it was all gone.\n",
    "Irretrievably, irrevocably gone.\n",
    "And in the warm moist shelter of the greenhouse, Saeng broke down and wept.\n",
    "It was already dusk when Saeng reached home. The wind was blowing harder,\n",
    "tearing off the last remnants of green in the chicory weeds that were growing out\n",
    "of the cracks in the sidewalk. As if oblivious to the cold, her mother was still out in\n",
    "the vegetable garden, digging up the last of the onions with a rusty trowel. She did\n",
    "not see Saeng until the girl had quietly knelt down next to her.\n",
    "Her smile of welcome warmed Saeng. “Ghup ma laio le? You’re back?” she said\n",
    "cheerfully. “Goodness, it’s past five. What took you so long? How did it go? Did\n",
    "you—?” Then she noticed the potted plant that Saeng was holding, its leaves\n",
    "quivering in the wind.\n",
    "Mrs. Panouvong uttered a small cry of surprise and delight. “Dok faeng-noi!”\n",
    "she said. “Where did you get it?”\n",
    "“I bought it,” Saeng answered, dreading her mother’s next question.\n",
    "“How much?”\n",
    "For answer Saeng handed her mother some coins.\n",
    "“That’s all?” Mrs. Panouvong said, appalled. “Oh, but I forgot! You and the\n",
    "Lambert boy ate Bee-Maags. . . .”\n",
    "10\n",
    "THE WINTER HIBISCUS\n",
    "“No, we didn’t, Mother,” Saeng said.\n",
    "“Then what else—?”\n",
    "“Nothing else. I paid over nineteen dollars for it.”\n",
    "“You what?” Her mother stared at her incredulously. “But how could you? All\n",
    "the seeds for this vegetable garden didn’t cost that much! You know how much\n",
    "we—” She paused, as she noticed the tearstains on her daughter’s cheeks and her\n",
    "puffy eyes.\n",
    "“What happened?” she asked, more gently.\n",
    "“I—I failed the test,” Saeng said.\n",
    "For a long moment Mrs. Panouvong said nothing. Saeng did not dare to look\n",
    "her mother in the eye. Instead, she stared at the hibiscus plant and nervously tore\n",
    "off a leaf, shredding it to bits.\n",
    "Her mother reached out and brushed the fragments of green off Saeng’s hands.\n",
    "“It’s a beautiful plant, this dok faeng-noi,” she finally said. “I’m glad you got it.”\n",
    "“It’s—it’s not a real one,” Saeng mumbled. “I mean, not like the kind we had\n",
    "at—at—” She found that she was still too shaky to say the words at home, lest she\n",
    "burst into tears again. “Not like the kind we had before,” she said.\n",
    "“I know,” her mother said quietly. “I’ve seen this kind blooming along the lake.\n",
    "Its flowers aren’t as pretty, but it’s strong enough to make it through the cold\n",
    "months here, this winter hibiscus. That’s what matters.”\n",
    " She tipped the pot and deftly eased the ball of soil out, balancing the rest of the\n",
    "plant in her other hand. “Look how rootbound it is, poor thing,” she said. “Let’s\n",
    "plant it, right now.”\n",
    "She went over to the corner of the vegetable patch and started to dig a hole in\n",
    "the ground. The soil was cold and hard, and she had trouble thrusting the shovel\n",
    "into it. Wisps of her gray hair trailed out in the breeze, and her slight frown\n",
    "deepened the wrinkles around her eyes. There was a frail, wiry beauty to her that\n",
    "touched Saeng deeply.\n",
    "“Here, let me help, Mother,” she offered, getting up and taking the shovel away\n",
    "from her.\n",
    "Mrs. Panouvong made no resistance. “I’ll bring in the hot peppers and bitter\n",
    "melons, then, and start dinner. How would you like an omelet with slices of the\n",
    "bitter melon?”\n",
    "“I’d love it,” Saeng said.\n",
    "Left alone in the garden, Saeng dug out a hole and carefully lowered the “winter\n",
    "hibiscus” into it. She could hear the sounds of cooking from the kitchen now, the\n",
    "beating of the eggs against a bowl, the sizzle of hot oil in the pan. The pungent\n",
    "smell of bitter melon wafted out, and Saeng’s mouth watered. It was a cultivated\n",
    "taste, she had discovered—none of her classmates or friends, not even Mrs.\n",
    "Lambert, liked it—this sharp, bitter melon that left a golden aftertaste on the\n",
    "tongue. But she had grown up eating it and, she admitted to herself, much\n",
    "preferred it to a Big Mac.\n",
    "11\n",
    "THE WINTER HIBISCUS\n",
    "The “winter hibiscus” was in the ground now, and Saeng tamped down the soil\n",
    "around it. Overhead, a flock of Canada geese flew by, their faint honks clear and—\n",
    "yes—familiar to Saeng now. Almost reluctantly, she realized that many of the\n",
    "things that she had thought of as strange before had become, through the quiet\n",
    "repetition of season upon season, almost familiar to her now. Like the geese. She\n",
    "lifted her head and watched as their distinctive V was etched against the evening\n",
    "sky, slowly fading into the distance.\n",
    "When they come back, Saeng vowed silently to herself, in the spring, when the\n",
    "snows melt and the geese return and this hibiscus is budding, then I will take that\n",
    "test again\n",
    "\"\"\".replace(\"\\n\", \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8c3e07e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' It was like walking into another world. A hot, moist world exploding with greenery. Huge flat leaves, delicate wisps of tendrils, ferns and fronds and vines of all shades and shapes grew in seemingly random profusion. “Over there, in the corner, the hibiscus. Is that what you mean?” The florist pointed at a leafy potted plant by the corner. There, in a shaft of the wan afternoon sunlight, was a single bloodred blossom, its five petals splayed back to reveal a long stamen tipped with yellow pollen. Saeng felt a shock of recognition so intense, it was almost visceral. “Saebba,” Saeng whispered. A saebba hedge, tall and lush, had surrounded their garden, its lush green leaves dotted with vermilion flowers. And sometimes after a monsoon rain, a blossom or two would have blown into the well, so that when she drew up the well water, she would find a red blossom floating in the bucket. Slowly, Saeng walked down the narrow aisle toward the hibiscus. Orchids, lanna bushes, oleanders, elephant ear begonias, and bougainvillea vines surrounded her. Plants that she had not even realized she had known but had forgotten drew her back into her childhood world. When she got to the hibiscus, she reached out and touched a petal gently. It felt smooth and cool, with a hint of velvet toward the center—just as she had known it would feel. 9 THE WINTER HIBISCUS And beside it was yet another old friend, a small shrub with waxy leaves and dainty flowers with purplish petals and white centers. “Madagascar periwinkle,” its tag announced. How strange to see it in a pot, Saeng thought. Back home it just grew wild, jutting out from the cracks in brick walls or between tiled roofs. There had been a patch of it by the little spirit house where she used to help her mother light the incense and candles to the spirit who guarded their home and their family. Sometimes she would casually pick a flower or two to leave on the offerings of fruit and rice left at the altar. And that rich, sweet scent—that was familiar, too. Saeng scanned the greenery around her and found a tall, gangly plant with exquisite little white blossoms on it. “Dok Malik,” she said, savoring the feel of the word on her tongue, even as she silently noted the English name on its tag, “jasmine.” One of the blossoms had fallen off, and carefully Saeng picked it up and smelled it. She closed her eyes and breathed in, deeply. The familiar fragrance filled her lungs, and Saeng could almost feel the light strands of her grandmother’s long gray hair, freshly washed, as she combed it out with the fine-toothed buffalo-horn comb. And when the sun had dried it, Saeng would help the gnarled old fingers knot the hair into a bun, then slip a dok Malik bud into it. Saeng looked at the white bud in her hand now, small and fragile. Gently, she closed her palm around it and held it tight. That, at least, she could hold on to. But where was the finetoothed comb? The hibiscus hedge? The well? Her gentle grandmother? A wave of loss so deep and strong that it stung Saeng’s eyes now swept over her. A blink, a channel switch, a boat ride in the night, and it was all gone. Irretrievably, irrevocably gone. And in the warm moist shelter of the greenhouse, Saeng broke down and wept. It was already dusk when Saeng reached home. The wind was blowing harder, tearing off the last remnants of green in the chicory weeds that were growing out of the cracks in the sidewalk. As if oblivious to the cold, her mother was still out in the vegetable garden, digging up the last of the onions with a rusty trowel. She did not see Saeng until the girl had quietly knelt down next to her. Her smile of welcome warmed Saeng. “Ghup ma laio le? You’re back?” she said cheerfully. “Goodness, it’s past five. What took you so long? How did it go? Did you—?” Then she noticed the potted plant that Saeng was holding, its leaves quivering in the wind. Mrs. Panouvong uttered a small cry of surprise and delight. “Dok faeng-noi!” she said. “Where did you get it?” “I bought it,” Saeng answered, dreading her mother’s next question. “How much?” For answer Saeng handed her mother some coins. “That’s all?” Mrs. Panouvong said, appalled. “Oh, but I forgot! You and the Lambert boy ate Bee-Maags. . . .” 10 THE WINTER HIBISCUS “No, we didn’t, Mother,” Saeng said. “Then what else—?” “Nothing else. I paid over nineteen dollars for it.” “You what?” Her mother stared at her incredulously. “But how could you? All the seeds for this vegetable garden didn’t cost that much! You know how much we—” She paused, as she noticed the tearstains on her daughter’s cheeks and her puffy eyes. “What happened?” she asked, more gently. “I—I failed the test,” Saeng said. For a long moment Mrs. Panouvong said nothing. Saeng did not dare to look her mother in the eye. Instead, she stared at the hibiscus plant and nervously tore off a leaf, shredding it to bits. Her mother reached out and brushed the fragments of green off Saeng’s hands. “It’s a beautiful plant, this dok faeng-noi,” she finally said. “I’m glad you got it.” “It’s—it’s not a real one,” Saeng mumbled. “I mean, not like the kind we had at—at—” She found that she was still too shaky to say the words at home, lest she burst into tears again. “Not like the kind we had before,” she said. “I know,” her mother said quietly. “I’ve seen this kind blooming along the lake. Its flowers aren’t as pretty, but it’s strong enough to make it through the cold months here, this winter hibiscus. That’s what matters.”  She tipped the pot and deftly eased the ball of soil out, balancing the rest of the plant in her other hand. “Look how rootbound it is, poor thing,” she said. “Let’s plant it, right now.” She went over to the corner of the vegetable patch and started to dig a hole in the ground. The soil was cold and hard, and she had trouble thrusting the shovel into it. Wisps of her gray hair trailed out in the breeze, and her slight frown deepened the wrinkles around her eyes. There was a frail, wiry beauty to her that touched Saeng deeply. “Here, let me help, Mother,” she offered, getting up and taking the shovel away from her. Mrs. Panouvong made no resistance. “I’ll bring in the hot peppers and bitter melons, then, and start dinner. How would you like an omelet with slices of the bitter melon?” “I’d love it,” Saeng said. Left alone in the garden, Saeng dug out a hole and carefully lowered the “winter hibiscus” into it. She could hear the sounds of cooking from the kitchen now, the beating of the eggs against a bowl, the sizzle of hot oil in the pan. The pungent smell of bitter melon wafted out, and Saeng’s mouth watered. It was a cultivated taste, she had discovered—none of her classmates or friends, not even Mrs. Lambert, liked it—this sharp, bitter melon that left a golden aftertaste on the tongue. But she had grown up eating it and, she admitted to herself, much preferred it to a Big Mac. 11 THE WINTER HIBISCUS The “winter hibiscus” was in the ground now, and Saeng tamped down the soil around it. Overhead, a flock of Canada geese flew by, their faint honks clear and— yes—familiar to Saeng now. Almost reluctantly, she realized that many of the things that she had thought of as strange before had become, through the quiet repetition of season upon season, almost familiar to her now. Like the geese. She lifted her head and watched as their distinctive V was etched against the evening sky, slowly fading into the distance. When they come back, Saeng vowed silently to herself, in the spring, when the snows melt and the geese return and this hibiscus is budding, then I will take that test again '"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c394ee43",
   "metadata": {},
   "outputs": [],
   "source": [
    "dalle_list = set()\n",
    "with open(\"./dalle_chall.txt\", \"r\") as fp:\n",
    "    for line in fp:\n",
    "        for word in word_tokenize(line.strip()):\n",
    "            dalle_list.add(word.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c4ba8828",
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tune_results = client.fine_tuning.jobs.retrieve(fine_tuning_job.id)\n",
    "ft_model = fine_tune_results.fine_tuned_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f34d7a1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'texts' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[43mtexts\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_features\u001b[39m(text: \u001b[38;5;28mstr\u001b[39m, score: \u001b[38;5;28mint\u001b[39m):\n\u001b[1;32m      5\u001b[0m     stopwords_set \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(stopwords\u001b[38;5;241m.\u001b[39mwords(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'texts' is not defined"
     ]
    }
   ],
   "source": [
    "text = texts[1]\n",
    "\n",
    "\n",
    "def get_features(text: str, score: int):\n",
    "    stopwords_set = set(stopwords.words(\"english\"))\n",
    "    text_blob = TextBlob(text)\n",
    "    prompt = text\n",
    "\n",
    "    # Best effort correcting text\n",
    "    corrected_text = str(text_blob.correct())\n",
    "    text = re.sub(r\"[^a-zA-Z\\s.,\\']\", \" \", text)\n",
    "    corrected_text = re.sub(r\"[^a-zA-Z\\s.,\\']\", \" \", corrected_text)\n",
    "    tokens = word_tokenize(corrected_text)\n",
    "    tokens_prev = word_tokenize(text)\n",
    "\n",
    "    # Estimate errors\n",
    "    num_errors = sum(1 for w1, w2 in zip(tokens, tokens_prev) if w1 != w2)\n",
    "\n",
    "    sentences = [word_tokenize(sent) for sent in sent_tokenize(text)]\n",
    "    text_blob = TextBlob(corrected_text)\n",
    "\n",
    "    features = {\"num_errors\": num_errors}\n",
    "\n",
    "    # Surface features\n",
    "    num_characters = len(text)\n",
    "    words = [word for word in tokens if len(word) > 1]\n",
    "    word_count = len(words)\n",
    "    average_word_length = sum(len(word) for word in words) / len(words)\n",
    "    num_sentences = len(sentences)\n",
    "    average_sentence_length = sum(len(sent) for sent in sentences) / len(sentences)\n",
    "    num_different_words = len(set(words))\n",
    "    num_of_stopwords = len([word for word in words if word in stopwords_set])\n",
    "\n",
    "    features.update(\n",
    "        {\n",
    "            \"num_characters\": num_characters,\n",
    "            \"word_count\": word_count,\n",
    "            \"average_word_length\": average_word_length,\n",
    "            \"num_sentences\": num_sentences,\n",
    "            \"average_sentence_length\": average_sentence_length,\n",
    "            \"num_different_words\": num_different_words,\n",
    "            \"num_of_stopwords\": num_of_stopwords,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    syllable_estimates = [syllables.estimate(word) for word in words]\n",
    "    syllable_count = sum(syllable_estimates)\n",
    "\n",
    "    # Readability\n",
    "    flesch_reading_ease = (\n",
    "        206.835\n",
    "        - 1.015 * (word_count / num_sentences)\n",
    "        - 84.6 * (syllable_count / word_count)\n",
    "    )\n",
    "    flesch_kincaid_grade_level = (\n",
    "        0.39 * (word_count / num_sentences)\n",
    "        + 11.8 * (syllable_count / word_count)\n",
    "        - 15.59\n",
    "    )\n",
    "\n",
    "    difficult_word_count = len([word for word in words if word not in dalle_list])\n",
    "    dalle_chall_readability = 0.1579 * (\n",
    "        difficult_word_count / word_count * 100\n",
    "    ) + 0.0496 * (word_count / num_sentences)\n",
    "    # Automated readability index\n",
    "    ari = (\n",
    "        4.71 * (num_characters / word_count)\n",
    "        + 0.5 * (word_count / num_sentences)\n",
    "        - 21.43\n",
    "    )\n",
    "\n",
    "    group_t = sentences[0:10]\n",
    "    group_m = sentences[len(sentences) // 2 - 5 : len(sentences) // 2 + 5]\n",
    "    group_b = sentences[-10:]\n",
    "    nsw = 0\n",
    "    for sent in group_t + group_m + group_b:\n",
    "        for word in sent:\n",
    "            if syllables.estimate(word) >= 3:\n",
    "                nsw += 1\n",
    "\n",
    "    smog = 1.043 * np.sqrt(nsw) * 30 / len(sentences) + 3.1291\n",
    "\n",
    "    # LIX\n",
    "    B = len([w for w in tokens if w[0].isupper() or len(w) == 1])\n",
    "    C = len([w for w in words if len(w) > 6])\n",
    "    lix = word_count / B + (C * 100) / word_count\n",
    "\n",
    "    wvi = np.log(word_count) / np.log(\n",
    "        2 - np.log(num_different_words) / np.log(word_count)\n",
    "    )\n",
    "    gunning_fog_index = 0.4 * ((word_count / num_sentences) + 100 * (nsw / word_count))\n",
    "    \n",
    "\n",
    "    features.update(\n",
    "        {\n",
    "            \"flesch_reading_ease\": flesch_reading_ease,\n",
    "            \"flesch_kincaid_grade_level\": flesch_kincaid_grade_level,\n",
    "            \"dalle_chall_readability\": dalle_chall_readability,\n",
    "            \"ari\": ari,\n",
    "            \"smog\": smog,\n",
    "            \"lix\": lix,\n",
    "            \"wvi\": wvi,\n",
    "            \"gunning_fog_index\": gunning_fog_index,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Lexical diversity\n",
    "    ttr = num_different_words / word_count\n",
    "    rs, word_counts = defaultdict(int), defaultdict(int)\n",
    "    for word in words:\n",
    "        word_counts[word] += 1\n",
    "    for _, r in word_counts.items():\n",
    "        rs[r] += 1\n",
    "\n",
    "    yule_k = 1e4 * (sum(r**2 * vr for r, vr in rs.items()) - word_count) / word_count**2\n",
    "\n",
    "    min_range, max_range, trials = 35, 50, 5\n",
    "    ns = np.arange(min_range, max_range + 1)\n",
    "    ttrs = []\n",
    "    for idx, sample_size in enumerate(ns):\n",
    "        ttr = 0\n",
    "        if sample_size <= len(words):\n",
    "            for trial in range(trials):\n",
    "                word_list = np.random.choice(words, sample_size, replace=False)\n",
    "                ttr += len(set(word_list)) / len(word_list)\n",
    "            ttrs.append(ttr / trials)\n",
    "    ttrs = np.array(ttrs)\n",
    "    A = np.vstack([2 * (1 - ttrs) / ns[0:len(ttrs)]]).T\n",
    "    y = ttrs**2\n",
    "    d = np.linalg.lstsq(A, y, rcond=None)[0]\n",
    "    d_estimate = d[0]\n",
    "    hapax_legomena = rs[1]\n",
    "\n",
    "    guiraud = num_different_words / np.sqrt(word_count)\n",
    "    advanced_guiraud = difficult_word_count / np.sqrt(word_count)\n",
    "    features.update(\n",
    "        {\n",
    "            \"ttr\": ttr,\n",
    "            \"yule_k\": yule_k,\n",
    "            \"d_estimate\": d_estimate,\n",
    "            \"hapax_legomena\": rs[1],\n",
    "            \"guiraud\": guiraud,\n",
    "            \"advanced_guiraud\": advanced_guiraud,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # POS tags\n",
    "    pos_tags = defaultdict(int)\n",
    "    total_pos_tags = 0\n",
    "    unique_pos_tags = 0\n",
    "    for _, tag in text_blob.tags:\n",
    "        if tag not in pos_tags:\n",
    "            unique_pos_tags += 1\n",
    "        pos_tags[tag] += 1\n",
    "        total_pos_tags += 1\n",
    "\n",
    "    pos_dist = {}\n",
    "    for tag, count in pos_tags.items():\n",
    "        pos_dist[tag] = count/total_pos_tags\n",
    "        \n",
    "    features[\"total_pos_tags\"] = total_pos_tags\n",
    "    features[\"unique_pos_tags\"] = unique_pos_tags\n",
    "    features.update(pos_dist)\n",
    "    features.update(pos_tags)\n",
    "    \n",
    "    # Discourse patterns\n",
    "    doc = nlp(corrected_text)\n",
    "    egrid = EntityGrid(doc)\n",
    "    (\n",
    "        local_coherence_PU,\n",
    "        local_coherence_PW,\n",
    "        local_coherence_PACC,\n",
    "        local_coherence_PU_dist,\n",
    "        local_coherence_PW_dist,\n",
    "        local_coherence_PACC_dist,\n",
    "    ) = get_local_coherence(egrid)\n",
    "    \n",
    "    features.update({\n",
    "        \"ss_transitions\": egrid.get_ss_transitions(),\n",
    "        \"so_transitions\": egrid.get_so_transitions(),\n",
    "        \"sx_transitions\": egrid.get_sx_transitions(),\n",
    "        \"sn_transitions\": egrid.get_sn_transitions(),\n",
    "        \"os_transitions\": egrid.get_os_transitions(),\n",
    "        \"oo_transitions\": egrid.get_oo_transitions(),\n",
    "        \"ox_transitions\": egrid.get_ox_transitions(),\n",
    "        \"on_transitions\": egrid.get_on_transitions(),\n",
    "        \"xs_transitions\": egrid.get_xs_transitions(),\n",
    "        \"xo_transitions\": egrid.get_xo_transitions(),\n",
    "        \"xx_transitions\": egrid.get_xx_transitions(),\n",
    "        \"xn_transitions\": egrid.get_xn_transitions(),\n",
    "        \"ns_transitions\": egrid.get_ns_transitions(),\n",
    "        \"no_transitions\": egrid.get_no_transitions(),\n",
    "        \"nx_transitions\": egrid.get_nx_transitions(),\n",
    "        \"nn_transitions\": egrid.get_nn_transitions(),\n",
    "        \"local_coherence_PU\": local_coherence_PU,\n",
    "        \"local_coherence_PW\": local_coherence_PW,\n",
    "        \"local_coherence_PACC\": local_coherence_PACC,\n",
    "        \"local_coherence_PU_dist\": local_coherence_PU_dist,\n",
    "        \"local_coherence_PW_dist\": local_coherence_PW_dist,\n",
    "        \"local_coherence_PACC_dist\": local_coherence_PACC_dist,\n",
    "    })\n",
    "\n",
    "    # CVA Features\n",
    "    \n",
    "    # Compute weight scores for the essay\n",
    "    fi = vectorizer.transform([text])\n",
    "    max_f = fi.max()\n",
    "    wi = (fi / max_f).toarray() * np.log(n / ni)\n",
    "\n",
    "\n",
    "    # Maximum similarity to the best score category\n",
    "    max_sim_best = np.dot(wi, content_vectors[max_rating])/(norm(wi)*norm(content_vectors[max_rating]))\n",
    "    max_sim_best = max_sim_best[0]\n",
    "\n",
    "    pattern_cosine = 0\n",
    "    val_cos = 0\n",
    "    max_cos = 0\n",
    "    for i in range(min_rating, max_rating + 1):\n",
    "        # Similarity between content vectors of category i and the essay vector\n",
    "        cos = np.dot(wi, content_vectors[i])/(norm(wi)*norm(content_vectors[i]))\n",
    "        pattern_cosine += i * cos[0]\n",
    "        \n",
    "        # This is for the val cosine, e.g. cos_4 + cos_3 - cos_2 - cos_1\n",
    "        if i < (min(scores) + max(scores)) // 2:\n",
    "            val_cos -= cos[0]\n",
    "        else:\n",
    "            val_cos += cos[0]\n",
    "\n",
    "        # We are also looking for the score category closest to the essay\n",
    "        if cos[0] >= max_cos:\n",
    "            max_cos_val = i\n",
    "            max_cos = cos[0]\n",
    "            \n",
    "    fsource = vectorizer.transform([source_text])\n",
    "    max_fsource = fsource.max()\n",
    "    wsource = (fi / max_fsource).toarray() * np.log(n / ni)\n",
    "    cos_source = np.dot(wi.squeeze(), wsource.squeeze())/(norm(wi)*norm(wsource))\n",
    "    \n",
    "    features.update({\n",
    "        \"max_cos_val\": max_cos_val,\n",
    "        \"max_sim_best\": max_sim_best,\n",
    "        \"pattern_cosine\": pattern_cosine,\n",
    "        \"val_cos\": val_cos,\n",
    "        \"similarity_source_text\": cos_source,\n",
    "    })\n",
    "    \n",
    "    # GPT4 fine-tuned model as a feature\n",
    "    prompt = prompt + '\\n\\n###\\n\\n'\n",
    "    res = client.completions.create(\n",
    "        model=ft_model,\n",
    "        prompt=prompt,\n",
    "        max_tokens=2, temperature=0)\n",
    "\n",
    "    gpt_score = int(re.sub(r\"[^0-9]\", \"\", res.choices[0].text)) % 10\n",
    "    features[\"gpt_score\"] = gpt_score\n",
    "    \n",
    "# These are experimental, based on spatial measurements\n",
    "#     dist = cosine_distances(embeddings)\n",
    "#     nn_distances = np.min(dist + np.diag(np.diag(dist) + 10), axis=1)\n",
    "#     avg_nn_distance = np.mean(nn_distances)\n",
    "#     max_nn_distance = np.max(nn_distances)\n",
    "#     min_nn_distance = np.min(nn_distances)\n",
    "#     r_distance = 2*np.sqrt(dist.shape[0])*avg_nn_distance\n",
    "#     cum_freq_dist_nn_dist = np.mean(nn_distances <= avg_nn_distance)\n",
    "#     givenness = []\n",
    "#     for i in range(2, len(embeddings)):\n",
    "#         x = embeddings[0:i]\n",
    "#         u, s, vh = np.linalg.svd(x)\n",
    "#         orthonormal_vector = vh[-1]\n",
    "#         givenness.append(np.dot(embeddings[i], orthonormal_vector))\n",
    "\n",
    "#     avg_givenness = np.mean(givenness)\n",
    "#     max_givenness = np.max(givenness)\n",
    "#     min_givenness = np.min(givenness)\n",
    "#     givenness_proj = []\n",
    "#     for i in range(2, len(embeddings)):\n",
    "#         x = np.array(embeddings[0:i])\n",
    "#         A = x.T\n",
    "#         b = embeddings[i]\n",
    "#         c = np.linalg.lstsq(A, b, rcond=None)[0]\n",
    "#         bw = A.dot(c)\n",
    "#         bwo = b - bw\n",
    "#         N = np.dot(b, bwo)\n",
    "#         G = np.dot(b, bw)\n",
    "#         givenness_proj.append(N / (G + N))\n",
    "\n",
    "#     avg_givenness_proj = np.mean(givenness_proj)\n",
    "#     min_givenness_proj = np.min(givenness_proj)\n",
    "#     max_givenness_proj = np.max(givenness_proj)\n",
    "#     max_min_giv_ratio = max_givenness / min_givenness\n",
    "#     centroid = np.sum(embeddings, axis=0) / len(embeddings)\n",
    "#     dist_to_centroid = []\n",
    "#     for embedding in embeddings:\n",
    "#         dist_to_centroid.append(np.dot(centroid, embedding))\n",
    "\n",
    "#     avg_dist_to_centroid = np.mean(dist_to_centroid)\n",
    "#     max_dist_to_centroid = np.max(dist_to_centroid)\n",
    "#     min_dist_to_centroid = np.min(dist_to_centroid)\n",
    "#     std_distance = np.sqrt(np.sum(np.sum(\n",
    "#         (embeddings - centroid)**2, axis=1))/len(embeddings))\n",
    "#     relative_distance = std_distance / max_dist_to_centroid\n",
    "#     det_dist = np.linalg.det(dist)\n",
    "\n",
    "#     features.update({\n",
    "#         \"avg_nn_distance\": avg_nn_distance,\n",
    "#         \"max_nn_distance\": max_nn_distance,\n",
    "#         \"min_nn_distance\": min_nn_distance,\n",
    "#         \"r_distance\": r_distance,\n",
    "#         \"cum_freq_dist_nn_dist\": cum_freq_dist_nn_dist,\n",
    "#         \"avg_givenness\": avg_givenness,\n",
    "#         \"max_givenness\": max_givenness,\n",
    "#         \"min_givenness\": min_givenness,\n",
    "#         \"max_min_giv_ratio\": max_min_giv_ratio,\n",
    "#         \"avg_givenness_proj\": avg_givenness_proj,\n",
    "#         \"min_givenness_proj\": min_givenness_proj,\n",
    "#         \"max_givenness_proj\": max_givenness_proj,\n",
    "#         \"max_min_giv_ratio\": max_min_giv_ratio,\n",
    "#         \"avg_dist_to_centroid\": avg_dist_to_centroid,\n",
    "#         \"max_dist_to_centroid\": max_dist_to_centroid,\n",
    "#         \"min_dist_to_centroid\": min_dist_to_centroid,\n",
    "#         \"std_distance\": std_distance,\n",
    "#         \"relative_distance\": relative_distance,\n",
    "#         \"det_dist\": det_dist,\n",
    "#     })\n",
    "    features[\"score\"] = score\n",
    "\n",
    "    return features\n",
    "\n",
    "# print(get_features(texts[0], scores[1]))\n",
    "args = list(zip(texts, scores))\n",
    "train_result = pqdm(args, get_features, n_jobs=8, argument_type=\"args\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ca588a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_results = []\n",
    "for idx, res in enumerate(train_result):\n",
    "    if type(res) != dict:\n",
    "        continue\n",
    "    train_results.append(res)\n",
    "        \n",
    "essay_set_features = pd.DataFrame(train_results).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3a395647",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = essay_set_features.drop([\"score\", \"gpt_score\"], axis=1), essay_set_features.score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "20f51ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestRegressor(\n",
    "    n_estimators=200, max_features=1/3, max_depth=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e685c0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def essay_metrics(clf, X, y_true):\n",
    "    y_pred = np.clip(np.round(clf.predict(X)), min_rating, max_rating)\n",
    "    qwk = quadratic_weighted_kappa(y_true, y_pred, min_rating=min_rating, max_rating=max_rating)\n",
    "    ea = np.sum(y_true == y_pred) / y_true.shape[0]\n",
    "    aa = np.sum((y_true - y_pred) <= 1) / y_true.shape[0]\n",
    "    return {\n",
    "        \"qwk\": qwk,\n",
    "        \"ea\": ea,\n",
    "        \"aa\": aa\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b4098e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert min_rating == y.min()\n",
    "assert max_rating == y.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f75e1f1",
   "metadata": {},
   "source": [
    "# DP Model Alone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ba4f5022",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "results = cross_validate(clf, X, y, scoring=essay_metrics, cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e2e29706",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8406405146493252, 0.7610062893081762, 1.0)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(results[\"test_qwk\"]), np.max(results[\"test_ea\"]), np.max(results[\"test_aa\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6576ed09",
   "metadata": {},
   "source": [
    "# GPT Model Alone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4331dce8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7464776849742681, 0.6735849056603773, 0.9974842767295597)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = essay_set_features.gpt_score\n",
    "y_true = essay_set_features.score\n",
    "qwk = quadratic_weighted_kappa(y_true, y_pred, min_rating=min_rating, max_rating=max_rating)\n",
    "ea = np.sum(y_true == y_pred) / y_true.shape[0]\n",
    "aa = np.sum((y_true - y_pred) <= 1) / y_true.shape[0]\n",
    "qwk, ea, aa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d582514",
   "metadata": {},
   "source": [
    "# Hybrid Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5bb33e5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.831983092638253, 0.7421383647798742, 1.0)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = essay_set_features.drop([\"score\"], axis=1), essay_set_features.score\n",
    "clf = RandomForestRegressor(\n",
    "    n_estimators=200, max_features=1/3, max_depth=12)\n",
    "results = cross_validate(clf, X, y, scoring=essay_metrics, cv=10)\n",
    "np.max(results[\"test_qwk\"]), np.max(results[\"test_ea\"]), np.max(results[\"test_aa\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2dc26f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
