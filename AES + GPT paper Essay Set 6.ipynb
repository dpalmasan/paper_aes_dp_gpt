{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f104690",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import syllables\n",
    "from collections import defaultdict\n",
    "from pqdm.processes import pqdm\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from textblob import TextBlob\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import spacy\n",
    "from numpy.linalg import norm\n",
    "from tqdm import tqdm\n",
    "\n",
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "\n",
    "client = OpenAI(api_key=os.environ[\"OPEN_AI_API_KEY\"])\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "ESSAY_SET = 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415ccb28",
   "metadata": {},
   "source": [
    "# Define QWK Metric functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "782592ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix(rater_a, rater_b, min_rating=None, max_rating=None):\n",
    "    \"\"\"\n",
    "    Returns the confusion matrix between rater's ratings\n",
    "    \"\"\"\n",
    "    assert(len(rater_a) == len(rater_b))\n",
    "    if min_rating is None:\n",
    "        min_rating = min(rater_a + rater_b)\n",
    "    if max_rating is None:\n",
    "        max_rating = max(rater_a + rater_b)\n",
    "    num_ratings = int(max_rating - min_rating + 1)\n",
    "    conf_mat = [[0 for i in range(num_ratings)]\n",
    "                for j in range(num_ratings)]\n",
    "    for a, b in zip(rater_a, rater_b):\n",
    "        conf_mat[a - min_rating][b - min_rating] += 1\n",
    "    return conf_mat\n",
    "\n",
    "\n",
    "def histogram(ratings, min_rating=None, max_rating=None):\n",
    "    \"\"\"\n",
    "    Returns the counts of each type of rating that a rater made\n",
    "    \"\"\"\n",
    "    if min_rating is None:\n",
    "        min_rating = min(ratings)\n",
    "    if max_rating is None:\n",
    "        max_rating = max(ratings)\n",
    "    num_ratings = int(max_rating - min_rating + 1)\n",
    "    hist_ratings = [0 for x in range(num_ratings)]\n",
    "    for r in ratings:\n",
    "        hist_ratings[r - min_rating] += 1\n",
    "    return hist_ratings\n",
    "\n",
    "\n",
    "def quadratic_weighted_kappa(rater_a, rater_b, min_rating=None, max_rating=None):\n",
    "    \"\"\"\n",
    "    Calculates the quadratic weighted kappa\n",
    "    quadratic_weighted_kappa calculates the quadratic weighted kappa\n",
    "    value, which is a measure of inter-rater agreement between two raters\n",
    "    that provide discrete numeric ratings.  Potential values range from -1\n",
    "    (representing complete disagreement) to 1 (representing complete\n",
    "    agreement).  A kappa value of 0 is expected if all agreement is due to\n",
    "    chance.\n",
    "\n",
    "    quadratic_weighted_kappa(rater_a, rater_b), where rater_a and rater_b\n",
    "    each correspond to a list of integer ratings.  These lists must have the\n",
    "    same length.\n",
    "\n",
    "    The ratings should be integers, and it is assumed that they contain\n",
    "    the complete range of possible ratings.\n",
    "\n",
    "    quadratic_weighted_kappa(X, min_rating, max_rating), where min_rating\n",
    "    is the minimum possible rating, and max_rating is the maximum possible\n",
    "    rating\n",
    "    \"\"\"\n",
    "    rater_a = np.array(rater_a, dtype=int)\n",
    "    rater_b = np.array(rater_b, dtype=int)\n",
    "    assert(len(rater_a) == len(rater_b))\n",
    "    if min_rating is None:\n",
    "        min_rating = min(min(rater_a), min(rater_b))\n",
    "    if max_rating is None:\n",
    "        max_rating = max(max(rater_a), max(rater_b))\n",
    "    conf_mat = confusion_matrix(rater_a, rater_b,\n",
    "                                min_rating, max_rating)\n",
    "    num_ratings = len(conf_mat)\n",
    "    num_scored_items = float(len(rater_a))\n",
    "\n",
    "    hist_rater_a = histogram(rater_a, min_rating, max_rating)\n",
    "    hist_rater_b = histogram(rater_b, min_rating, max_rating)\n",
    "\n",
    "    numerator = 0.0\n",
    "    denominator = 0.0\n",
    "\n",
    "    for i in range(num_ratings):\n",
    "        for j in range(num_ratings):\n",
    "            expected_count = (hist_rater_a[i] * hist_rater_b[j]\n",
    "                              / num_scored_items)\n",
    "            d = pow(i - j, 2.0) / pow(num_ratings - 1, 2.0)\n",
    "            numerator += d * conf_mat[i][j] / num_scored_items\n",
    "            denominator += d * expected_count / num_scored_items\n",
    "\n",
    "    return 1.0 - numerator / denominator\n",
    "\n",
    "\n",
    "def linear_weighted_kappa(rater_a, rater_b, min_rating=None, max_rating=None):\n",
    "    \"\"\"\n",
    "    Calculates the linear weighted kappa\n",
    "    linear_weighted_kappa calculates the linear weighted kappa\n",
    "    value, which is a measure of inter-rater agreement between two raters\n",
    "    that provide discrete numeric ratings.  Potential values range from -1\n",
    "    (representing complete disagreement) to 1 (representing complete\n",
    "    agreement).  A kappa value of 0 is expected if all agreement is due to\n",
    "    chance.\n",
    "\n",
    "    linear_weighted_kappa(rater_a, rater_b), where rater_a and rater_b\n",
    "    each correspond to a list of integer ratings.  These lists must have the\n",
    "    same length.\n",
    "\n",
    "    The ratings should be integers, and it is assumed that they contain\n",
    "    the complete range of possible ratings.\n",
    "\n",
    "    linear_weighted_kappa(X, min_rating, max_rating), where min_rating\n",
    "    is the minimum possible rating, and max_rating is the maximum possible\n",
    "    rating\n",
    "    \"\"\"\n",
    "    assert(len(rater_a) == len(rater_b))\n",
    "    if min_rating is None:\n",
    "        min_rating = min(rater_a + rater_b)\n",
    "    if max_rating is None:\n",
    "        max_rating = max(rater_a + rater_b)\n",
    "    conf_mat = confusion_matrix(rater_a, rater_b,\n",
    "                                min_rating, max_rating)\n",
    "    num_ratings = len(conf_mat)\n",
    "    num_scored_items = float(len(rater_a))\n",
    "\n",
    "    hist_rater_a = histogram(rater_a, min_rating, max_rating)\n",
    "    hist_rater_b = histogram(rater_b, min_rating, max_rating)\n",
    "\n",
    "    numerator = 0.0\n",
    "    denominator = 0.0\n",
    "\n",
    "    for i in range(num_ratings):\n",
    "        for j in range(num_ratings):\n",
    "            expected_count = (hist_rater_a[i] * hist_rater_b[j]\n",
    "                              / num_scored_items)\n",
    "            d = abs(i - j) / float(num_ratings - 1)\n",
    "            numerator += d * conf_mat[i][j] / num_scored_items\n",
    "            denominator += d * expected_count / num_scored_items\n",
    "\n",
    "    return 1.0 - numerator / denominator\n",
    "\n",
    "\n",
    "def kappa(rater_a, rater_b, min_rating=None, max_rating=None):\n",
    "    \"\"\"\n",
    "    Calculates the kappa\n",
    "    kappa calculates the kappa\n",
    "    value, which is a measure of inter-rater agreement between two raters\n",
    "    that provide discrete numeric ratings.  Potential values range from -1\n",
    "    (representing complete disagreement) to 1 (representing complete\n",
    "    agreement).  A kappa value of 0 is expected if all agreement is due to\n",
    "    chance.\n",
    "\n",
    "    kappa(rater_a, rater_b), where rater_a and rater_b\n",
    "    each correspond to a list of integer ratings.  These lists must have the\n",
    "    same length.\n",
    "\n",
    "    The ratings should be integers, and it is assumed that they contain\n",
    "    the complete range of possible ratings.\n",
    "\n",
    "    kappa(X, min_rating, max_rating), where min_rating\n",
    "    is the minimum possible rating, and max_rating is the maximum possible\n",
    "    rating\n",
    "    \"\"\"\n",
    "    assert(len(rater_a) == len(rater_b))\n",
    "    if min_rating is None:\n",
    "        min_rating = min(rater_a + rater_b)\n",
    "    if max_rating is None:\n",
    "        max_rating = max(rater_a + rater_b)\n",
    "    conf_mat = confusion_matrix(rater_a, rater_b,\n",
    "                                min_rating, max_rating)\n",
    "    num_ratings = len(conf_mat)\n",
    "    num_scored_items = float(len(rater_a))\n",
    "\n",
    "    hist_rater_a = histogram(rater_a, min_rating, max_rating)\n",
    "    hist_rater_b = histogram(rater_b, min_rating, max_rating)\n",
    "\n",
    "    numerator = 0.0\n",
    "    denominator = 0.0\n",
    "\n",
    "    for i in range(num_ratings):\n",
    "        for j in range(num_ratings):\n",
    "            expected_count = (hist_rater_a[i] * hist_rater_b[j]\n",
    "                              / num_scored_items)\n",
    "            if i == j:\n",
    "                d = 0.0\n",
    "            else:\n",
    "                d = 1.0\n",
    "            numerator += d * conf_mat[i][j] / num_scored_items\n",
    "            denominator += d * expected_count / num_scored_items\n",
    "\n",
    "    return 1.0 - numerator / denominator\n",
    "\n",
    "\n",
    "def mean_quadratic_weighted_kappa(kappas, weights=None):\n",
    "    \"\"\"\n",
    "    Calculates the mean of the quadratic\n",
    "    weighted kappas after applying Fisher's r-to-z transform, which is\n",
    "    approximately a variance-stabilizing transformation.  This\n",
    "    transformation is undefined if one of the kappas is 1.0, so all kappa\n",
    "    values are capped in the range (-0.999, 0.999).  The reverse\n",
    "    transformation is then applied before returning the result.\n",
    "\n",
    "    mean_quadratic_weighted_kappa(kappas), where kappas is a vector of\n",
    "    kappa values\n",
    "\n",
    "    mean_quadratic_weighted_kappa(kappas, weights), where weights is a vector\n",
    "    of weights that is the same size as kappas.  Weights are applied in the\n",
    "    z-space\n",
    "    \"\"\"\n",
    "    kappas = np.array(kappas, dtype=float)\n",
    "    if weights is None:\n",
    "        weights = np.ones(np.shape(kappas))\n",
    "    else:\n",
    "        weights = weights / np.mean(weights)\n",
    "\n",
    "    # ensure that kappas are in the range [-.999, .999]\n",
    "    kappas = np.array([min(x, .999) for x in kappas])\n",
    "    kappas = np.array([max(x, -.999) for x in kappas])\n",
    "\n",
    "    z = 0.5 * np.log((1 + kappas) / (1 - kappas)) * weights\n",
    "    z = np.mean(z)\n",
    "    return (np.exp(2 * z) - 1) / (np.exp(2 * z) + 1)\n",
    "\n",
    "\n",
    "def weighted_mean_quadratic_weighted_kappa(solution, submission):\n",
    "    predicted_score = submission[submission.columns[-1]].copy()\n",
    "    predicted_score.name = \"predicted_score\"\n",
    "    if predicted_score.index[0] == 0:\n",
    "        predicted_score = predicted_score[:len(solution)]\n",
    "        predicted_score.index = solution.index\n",
    "    combined = solution.join(predicted_score, how=\"left\")\n",
    "    groups = combined.groupby(by=\"essay_set\")\n",
    "    kappas = [quadratic_weighted_kappa(group[1][\"essay_score\"], group[1][\"predicted_score\"]) for group in groups]\n",
    "    weights = [group[1][\"essay_weight\"].irow(0) for group in groups]\n",
    "    return mean_quadratic_weighted_kappa(kappas, weights=weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d4bafaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "UNIVERSAL_NOUN_TAGS = set([u\"NOUN\", u\"PRON\", u\"PROPN\"])\n",
    "\n",
    "ordered_transitions = [\n",
    "    u\"SS\",\n",
    "    u\"SO\",\n",
    "    u\"SX\",\n",
    "    u\"S-\",\n",
    "    u\"OS\",\n",
    "    u\"OO\",\n",
    "    u\"OX\",\n",
    "    u\"O-\",\n",
    "    u\"XS\",\n",
    "    u\"XO\",\n",
    "    u\"XX\",\n",
    "    u\"X-\",\n",
    "    u\"-S\",\n",
    "    u\"-O\",\n",
    "    u\"-X\",\n",
    "    u\"--\",\n",
    "]\n",
    "\n",
    "\n",
    "def dependency_mapping(dep: str) -> str:\n",
    "    \"\"\"Map dependency tag to entity grid tag.\n",
    "\n",
    "    We consider the notation provided in :cite:`barzilay2008modeling`:\n",
    "\n",
    "    +-----------+-----------------------------------+\n",
    "    | EGrid Tag | Dependency Tag                    |\n",
    "    +===========+===================================+\n",
    "    | S         | nsub, csubj, csubjpass, dsubjpass |\n",
    "    +-----------+-----------------------------------+\n",
    "    | O         | iobj, obj, pobj, dobj             |\n",
    "    +-----------+-----------------------------------+\n",
    "    | X         | For any other dependency tag      |\n",
    "    +-----------+-----------------------------------+\n",
    "\n",
    "    :param dep: Dependency tag\n",
    "    :type dep: string\n",
    "    :return: EGrid tag\n",
    "    :rtype: string\n",
    "    \"\"\"\n",
    "    if dep in {u\"nsubj\", u\"csubj\", u\"csubjpass\", u\"dsubjpass\"}:\n",
    "        return u\"S\"\n",
    "    if dep in {u\"iobj\", u\"obj\", u\"pobj\", u\"dobj\"}:\n",
    "        return u\"O\"\n",
    "\n",
    "    return \"X\"\n",
    "\n",
    "\n",
    "class EntityGrid(object):\n",
    "    \"\"\"Entity grid class.\n",
    "\n",
    "    Class Entity Grid, creates an entity grid from a doc, which is output of\n",
    "    applying spacy.nlp(text) to a text. Thus, this class depends on spacy\n",
    "    module. It only supports 2-transitions entity grid.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, doc):\n",
    "        \"\"\"Construct EntityGrid object.\"\"\"\n",
    "        # Initialization\n",
    "        entity_map = dict()\n",
    "        entity_grid = dict()\n",
    "        i = 1\n",
    "        entity_map[\"s%d\" % i] = []\n",
    "        entity_features = {\n",
    "            u\"SS\": 0,\n",
    "            u\"SO\": 0,\n",
    "            u\"SX\": 0,\n",
    "            u\"S-\": 0,\n",
    "            u\"OS\": 0,\n",
    "            u\"OO\": 0,\n",
    "            u\"OX\": 0,\n",
    "            u\"O-\": 0,\n",
    "            u\"XS\": 0,\n",
    "            u\"XO\": 0,\n",
    "            u\"XX\": 0,\n",
    "            u\"X-\": 0,\n",
    "            u\"-S\": 0,\n",
    "            u\"-O\": 0,\n",
    "            u\"-X\": 0,\n",
    "            u\"--\": 0,\n",
    "        }\n",
    "\n",
    "\n",
    "        n_sent = len(list(doc.sents))\n",
    "\n",
    "        # To get coherence measurements we need at least 2 sentences\n",
    "        if n_sent < 2:\n",
    "            raise RuntimeError(\n",
    "                \"Entity grid needs at least two sentences, found: {}\".format(\n",
    "                    n_sent\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # For each sentence, get dependencies and its grammatical role\n",
    "        for sent in doc.sents:\n",
    "            for token in sent:\n",
    "                if token.pos_ in UNIVERSAL_NOUN_TAGS:\n",
    "                    entity_map[\"s%d\" % i].append(\n",
    "                        (token.text.upper(), token.dep_)\n",
    "                    )\n",
    "                    if token.text.upper() not in entity_grid:\n",
    "                        entity_grid[token.text.upper()] = [u\"-\"] * n_sent\n",
    "            i += 1\n",
    "            entity_map[\"s%d\" % i] = []\n",
    "\n",
    "        # Last iteration will create an extra element, so I remove it.\n",
    "        entity_map.pop(\"s%d\" % i)\n",
    "\n",
    "        # Fill entity grid\n",
    "        for i in range(n_sent):\n",
    "            sentence = \"s%d\" % (i + 1)\n",
    "            for entity, dep in entity_map[sentence]:\n",
    "                if entity_grid[entity][i] == u\"-\":\n",
    "                    entity_grid[entity][i] = dependency_mapping(dep)\n",
    "                elif dependency_mapping(dep) == u\"S\":\n",
    "                    entity_grid[entity][i] = dependency_mapping(dep)\n",
    "                elif (\n",
    "                    dependency_mapping(dep) == u\"O\"\n",
    "                    and entity_grid[entity][i] == u\"X\"\n",
    "                ):\n",
    "                    entity_grid[entity][i] = dependency_mapping(dep)\n",
    "\n",
    "        # Compute feature vector, we consider transitions of length 2\n",
    "        total_transitions = (n_sent - 1) * len(entity_grid.keys())\n",
    "\n",
    "        for entity in entity_grid:\n",
    "            for i in range(n_sent - 1):\n",
    "                # Transition type found (e.g. S-)\n",
    "                transition = (\n",
    "                    entity_grid[entity][i] + entity_grid[entity][i + 1]\n",
    "                )\n",
    "\n",
    "                # Adding 1 to transition count\n",
    "                entity_features[transition] += 1\n",
    "\n",
    "        for prob in entity_features:\n",
    "            if total_transitions != 0:\n",
    "                entity_features[prob] /= float(total_transitions)\n",
    "            else:\n",
    "                entity_features[prob] = 0.0\n",
    "\n",
    "        self.__grid = entity_grid\n",
    "        self.__n_sent = n_sent\n",
    "        self.__prob = entity_features\n",
    "\n",
    "    def get_ss_transitions(self) -> float:\n",
    "        \"\"\"Get SS transitions.\n",
    "\n",
    "        :return: Ratio of transitions\n",
    "        :rtype: float\n",
    "        \"\"\"\n",
    "        return self.__prob[u\"SS\"]\n",
    "\n",
    "    def get_so_transitions(self) -> float:\n",
    "        \"\"\"Get SO transitions.\n",
    "\n",
    "        :return: Ratio of transitions\n",
    "        :rtype: float\n",
    "        \"\"\"\n",
    "        return self.__prob[u\"SO\"]\n",
    "\n",
    "    def get_sx_transitions(self) -> float:\n",
    "        \"\"\"Get SX transitions.\n",
    "\n",
    "        :return: Ratio of transitions\n",
    "        :rtype: float\n",
    "        \"\"\"\n",
    "        return self.__prob[u\"SX\"]\n",
    "\n",
    "    def get_sn_transitions(self) -> float:\n",
    "        \"\"\"Get S- transitions.\n",
    "\n",
    "        :return: Ratio of transitions\n",
    "        :rtype: float\n",
    "        \"\"\"\n",
    "        return self.__prob[u\"S-\"]\n",
    "\n",
    "    def get_os_transitions(self) -> float:\n",
    "        \"\"\"Get OS transitions.\n",
    "\n",
    "        :return: Ratio of transitions\n",
    "        :rtype: float\n",
    "        \"\"\"\n",
    "        return self.__prob[u\"OS\"]\n",
    "\n",
    "    def get_oo_transitions(self) -> float:\n",
    "        \"\"\"Get OO transitions.\n",
    "\n",
    "        :return: Ratio of transitions\n",
    "        :rtype: float\n",
    "        \"\"\"\n",
    "        return self.__prob[u\"OO\"]\n",
    "\n",
    "    def get_ox_transitions(self) -> float:\n",
    "        \"\"\"Get OX transitions.\n",
    "\n",
    "        :return: Ratio of transitions\n",
    "        :rtype: float\n",
    "        \"\"\"\n",
    "        return self.__prob[u\"OX\"]\n",
    "\n",
    "    def get_on_transitions(self) -> float:\n",
    "        \"\"\"Get O- transitions.\n",
    "\n",
    "        :return: Ratio of transitions\n",
    "        :rtype: float\n",
    "        \"\"\"\n",
    "        return self.__prob[u\"O-\"]\n",
    "\n",
    "    def get_xs_transitions(self) -> float:\n",
    "        \"\"\"Get XS transitions.\n",
    "\n",
    "        :return: Ratio of transitions\n",
    "        :rtype: float\n",
    "        \"\"\"\n",
    "        return self.__prob[u\"XS\"]\n",
    "\n",
    "    def get_xo_transitions(self) -> float:\n",
    "        \"\"\"Get XO transitions.\n",
    "\n",
    "        :return: Ratio of transitions\n",
    "        :rtype: float\n",
    "        \"\"\"\n",
    "        return self.__prob[u\"XO\"]\n",
    "\n",
    "    def get_xx_transitions(self) -> float:\n",
    "        \"\"\"Get XX transitions.\n",
    "\n",
    "        :return: Ratio of transitions\n",
    "        :rtype: float\n",
    "        \"\"\"\n",
    "        return self.__prob[u\"XX\"]\n",
    "\n",
    "    def get_xn_transitions(self) -> float:\n",
    "        \"\"\"Get X- transitions.\n",
    "\n",
    "        :return: Ratio of transitions\n",
    "        :rtype: float\n",
    "        \"\"\"\n",
    "        return self.__prob[u\"X-\"]\n",
    "\n",
    "    def get_ns_transitions(self) -> float:\n",
    "        \"\"\"Get -S transitions.\n",
    "\n",
    "        :return: Ratio of transitions\n",
    "        :rtype: float\n",
    "        \"\"\"\n",
    "        return self.__prob[u\"-S\"]\n",
    "\n",
    "    def get_no_transitions(self) -> float:\n",
    "        \"\"\"Get -O transitions.\n",
    "\n",
    "        :return: Ratio of transitions\n",
    "        :rtype: float\n",
    "        \"\"\"\n",
    "        return self.__prob[u\"-O\"]\n",
    "\n",
    "    def get_nx_transitions(self) -> float:\n",
    "        \"\"\"Get -X transitions.\n",
    "\n",
    "        :return: Ratio of transitions\n",
    "        :rtype: float\n",
    "        \"\"\"\n",
    "        return self.__prob[u\"-X\"]\n",
    "\n",
    "    def get_nn_transitions(self) -> float:\n",
    "        \"\"\"Get -- transitions.\n",
    "\n",
    "        :return: Ratio of transitions\n",
    "        :rtype: float\n",
    "        \"\"\"\n",
    "        return self.__prob[u\"--\"]\n",
    "\n",
    "    def get_egrid(self) -> dict:\n",
    "        \"\"\"Return obtained entity grid (for debugging purposes).\n",
    "\n",
    "        :return: entity grid represented as a dict\n",
    "        :rtype: dict\n",
    "        \"\"\"\n",
    "        return self.__grid\n",
    "\n",
    "    def get_sentence_count(self) -> int:\n",
    "        \"\"\"Return sentence count obtained while processing.\n",
    "\n",
    "        :return: Number of sentences\n",
    "        :rtype: int\n",
    "        \"\"\"\n",
    "        return self.__n_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10224389",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighting_syntactic_role(entity_role: str) -> int:\n",
    "    \"\"\"Return weight given an entity grammatical role.\n",
    "\n",
    "    Weighting scheme for syntactic role of an entity. This uses the heuristic\n",
    "    from :cite:`guinaudeau2013graph`, which is:\n",
    "\n",
    "    +-----------+--------+\n",
    "    | EGrid Tag | Weight |\n",
    "    +===========+========+\n",
    "    | S         | 3      |\n",
    "    +-----------+--------+\n",
    "    | O         | 2      |\n",
    "    +-----------+--------+\n",
    "    | X         | 1      |\n",
    "    +-----------+--------+\n",
    "    | dash      | 0      |\n",
    "    +-----------+--------+\n",
    "\n",
    "    :param entity_role: Entity grammatical role (S, O, X, -)\n",
    "    :type entity_role: string\n",
    "    :return: Role weight\n",
    "    :rtype: int\n",
    "    \"\"\"\n",
    "    if entity_role == u\"S\":\n",
    "        return 3\n",
    "    elif entity_role == u\"O\":\n",
    "        return 2\n",
    "    elif entity_role == u\"X\":\n",
    "        return 1\n",
    "\n",
    "    return 0\n",
    "\n",
    "\n",
    "def get_local_coherence(egrid: EntityGrid) -> [float, float, float, float]:\n",
    "    \"\"\"Get local coherence from entity grid.\n",
    "\n",
    "    This method gets the coherence value using all the approaches described\n",
    "    in :cite:`guinaudeau2013graph`. This include:\n",
    "\n",
    "    * local_coherence_PU\n",
    "    * local_coherence_PW\n",
    "    * local_coherence_PACC\n",
    "    * local_coherence_PU_dist\n",
    "    * local_coherence_PW_dist\n",
    "    * local_coherence_PACC_dist\n",
    "\n",
    "    :param egrid: An EntityGrid object.\n",
    "    :type egrid: EntityGrid\n",
    "    :return: Local coherence based on different heuristics\n",
    "    :rtype: tuple of floats\n",
    "    \"\"\"\n",
    "    n_sent = egrid.get_sentence_count()\n",
    "\n",
    "    # If entity grid is not valid\n",
    "    if n_sent < 2:\n",
    "        return (0.0, 0.0, 0.0, 0.0, 0.0, 0.0)\n",
    "\n",
    "    PW = [[0] * n_sent for i in range(n_sent)]\n",
    "\n",
    "    # Weight Matrix for PACC, syntactic information is accounted for by\n",
    "    # integrating the edges of the bipartite graph\n",
    "    W = [[0] * n_sent for i in range(n_sent)]\n",
    "\n",
    "    grid = egrid.get_egrid()\n",
    "    for entity in grid:\n",
    "        for i in range(n_sent):\n",
    "            for j in range(i + 1, n_sent):\n",
    "                if grid[entity][i] != u\"-\" and grid[entity][j] != u\"-\":\n",
    "                    PW[i][j] += 1\n",
    "                    W[i][j] += weighting_syntactic_role(\n",
    "                        grid[entity][i]\n",
    "                    ) * weighting_syntactic_role(grid[entity][j])\n",
    "\n",
    "    PU = [list(map(lambda x: x != 0, PWi)) for PWi in PW]\n",
    "\n",
    "    local_coherence_PU = 0.0\n",
    "    local_coherence_PW = 0.0\n",
    "    local_coherence_PACC = 0.0\n",
    "    for i in range(n_sent):\n",
    "        local_coherence_PW += sum(PW[i])\n",
    "        local_coherence_PU += sum(PU[i])\n",
    "        local_coherence_PACC += sum(W[i])\n",
    "\n",
    "    local_coherence_PW /= n_sent\n",
    "    local_coherence_PU /= n_sent\n",
    "    local_coherence_PACC /= n_sent\n",
    "\n",
    "    # Weighting projection graphs\n",
    "    PU_weighted = list(PU)\n",
    "    PW_weighted = list(PW)\n",
    "    PACC_weighted = list(W)\n",
    "    for i in range(n_sent):\n",
    "        for j in range(i + 1, n_sent):\n",
    "            PU_weighted[i][j] = PU[i][j] / float(j - i)\n",
    "            PW_weighted[i][j] = PW[i][j] / float(j - i)\n",
    "            PACC_weighted[i][j] = W[i][j] / float(j - i)\n",
    "\n",
    "    local_coherence_PU_dist = 0.0\n",
    "    local_coherence_PW_dist = 0.0\n",
    "    local_coherence_PACC_dist = 0.0\n",
    "    for i in range(n_sent):\n",
    "        local_coherence_PW_dist += sum(PW_weighted[i])\n",
    "        local_coherence_PU_dist += sum(PU_weighted[i])\n",
    "        local_coherence_PACC_dist += sum(PACC_weighted[i])\n",
    "\n",
    "    local_coherence_PW_dist /= n_sent\n",
    "    local_coherence_PU_dist /= n_sent\n",
    "    local_coherence_PACC_dist /= n_sent\n",
    "    return (\n",
    "        local_coherence_PU,\n",
    "        local_coherence_PW,\n",
    "        local_coherence_PACC,\n",
    "        local_coherence_PU_dist,\n",
    "        local_coherence_PW_dist,\n",
    "        local_coherence_PACC_dist,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bed78faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\n",
    "    f\"./training_set_rel3.tsv\",\n",
    "    sep=\"\\t\",\n",
    "    encoding=\"ISO-8859-1\",\n",
    "    usecols=[\"essay_id\", \"essay_set\", \"essay\", \"domain1_score\", \"domain2_score\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af931dec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({2: 405, 3: 817, 4: 367, 1: 167, 0: 44})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "df = df[df.essay_set == ESSAY_SET]\n",
    "Counter(df.domain1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3875b716",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_rating, max_rating = int(df.domain1_score.min()), int(df.domain1_score.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b69e1cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ft = pd.DataFrame({\n",
    "    \"prompt\": df.essay.tolist(),\n",
    "    \"completion\": df.domain1_score.tolist()})\n",
    "\n",
    "df_ft.to_json(f\"essay_set{ESSAY_SET}.jsonl\", orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a7d1079",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>completion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>There were many obstacles that the builders fa...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Him from the start, there would have been many...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The builders of the Empire State Building face...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>In the passage The Mooring Mast by Marcia Amid...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The builders of the Empire State Building face...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              prompt  completion\n",
       "0  There were many obstacles that the builders fa...           2\n",
       "1  Him from the start, there would have been many...           3\n",
       "2  The builders of the Empire State Building face...           4\n",
       "3  In the passage The Mooring Mast by Marcia Amid...           1\n",
       "4  The builders of the Empire State Building face...           3"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ft.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e73f247",
   "metadata": {},
   "source": [
    "# Generate data for Fine Tuning\n",
    "\n",
    "We run the following tool:\n",
    "\n",
    "`openai tools fine_tunes.prepare_data -f essay_set6.jsonl -q`\n",
    "\n",
    "This will generate two data splits for the fine-tuning. One for training and other for validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ddc38e9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FineTuningJob(id='ftjob-jcg8J3u38NZhMhFxqoyakQDS', created_at=1721591701, error=Error(code=None, message=None, param=None), fine_tuned_model=None, finished_at=None, hyperparameters=Hyperparameters(n_epochs='auto', batch_size='auto', learning_rate_multiplier='auto'), model='babbage-002', object='fine_tuning.job', organization_id='org-JihYzTh2GjJjoPtZZ0kQdsbr', result_files=[], seed=189849559, status='validating_files', trained_tokens=None, training_file='file-jAqUSJPrHJVYA08xF3emW57W', validation_file='file-ROwYPwST2kEUcgcActpQNAkP', estimated_finish=None, integrations=[], user_provided_suffix=None)\n"
     ]
    }
   ],
   "source": [
    "train_file = client.files.create(file=open(f\"essay_set{ESSAY_SET}_prepared_train.jsonl\", \"rb\"), purpose=\"fine-tune\")\n",
    "valid_file = client.files.create(file=open(f\"essay_set{ESSAY_SET}_prepared_valid.jsonl\", \"rb\"), purpose=\"fine-tune\")\n",
    "fine_tuning_job = client.fine_tuning.jobs.create(training_file=train_file.id, validation_file=valid_file.id, model=\"babbage-002\")\n",
    "print(fine_tuning_job)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8695dcf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts, scores = (\n",
    "    df.essay.tolist(),\n",
    "    df.domain1_score.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "52762763",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(min_df=10)\n",
    "\n",
    "doc_term = vectorizer.fit_transform(df.essay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d0d9b0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting CVA features https://files.eric.ed.gov/fulltext/ED525309.pdf\n",
    "\n",
    "content_vectors = {}\n",
    "n = df.shape[0]\n",
    "ni = doc_term.getnnz(axis=0)\n",
    "for i in range(min_rating, max_rating + 1):\n",
    "    essay_score_cat = df[df.domain1_score == i]\n",
    "    freqs = vectorizer.transform(essay_score_cat.essay)\n",
    "    \n",
    "    # Get frequencies score categories\n",
    "    fis = freqs.sum(axis=0)\n",
    "    fis = np.asarray(fis).reshape(-1)\n",
    "    try:\n",
    "        max_fs = freqs.max()\n",
    "    except:\n",
    "        print(i)\n",
    "        raise\n",
    "    content_vectors[i] = fis/max_fs * np.log(n / ni)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3b724582",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1107,)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content_vectors[3].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3287935e",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_text = \"\"\"\n",
    "When the Empire State Building was conceived, it was planned as the world’s tallest building, taller even than the new Chrysler Building that was being constructed at Forty-second Street and Lexington Avenue in New York. At seventy-seven stories, it was the tallest building before the Empire State began construction, and Al Smith was determined to outstrip it in height.\n",
    "The architect building the Chrysler Building, however, had a trick up his sleeve. He secretly constructed a 185-foot spire inside the building, and then shocked the public and the media by hoisting it up to the top of the Chrysler Building, bringing it to a height of 1,046 feet, 46 feet taller than the originally announced height of the Empire State Building.\n",
    "Al Smith realized that he was close to losing the title of world’s tallest building, and on December 11, 1929, he announced that the Empire State would now reach the height of 1,250 feet. He would add a top or a hat to the building that would be even more distinctive than any other building in the city. John Tauranac describes the plan:\n",
    "[The top of the Empire State Building] would be more than ornamental, more than a spire or dome or a pyramid put there to add a desired few feet to the height of the building or to mask something as mundane as a water tank. Their top, they said, would serve a higher calling. The Empire State Building would be equipped for an age of transportation that was then only the dream of aviation pioneers.\n",
    "This dream of the aviation pioneers was travel by dirigible, or zeppelin, and the Empire State Building was going to have a mooring mast at its top for docking these new airships, which would accommodate passengers on already existing transatlantic routes and new routes that were yet to come.\n",
    "The Age of Dirigibles\n",
    "By the 1920s, dirigibles were being hailed as the transportation of the future. Also known today as blimps, dirigibles were actually enormous steel-framed balloons, with envelopes of cotton fabric filled with hydrogen and helium to make them lighter than air. Unlike a balloon, a dirigible could be maneuvered by the use of propellers and rudders, and passengers could ride in the gondola, or enclosed compartment, under the balloon.\n",
    "Dirigibles had a top speed of eighty miles per hour, and they could cruise at seventy miles per hour for thousands of miles without needing refueling. Some were as long as one thousand feet, the same length as four blocks in New York City. The one obstacle to their expanded use in New York City was the lack of a suitable landing area. Al Smith saw an opportunity for his Empire State Building: A mooring mast added to the top of the building would allow dirigibles to anchor there for several hours for refueling or service, and to let passengers off and on. Dirigibles were docked by means of an electric winch, which hauled in a line from the front of the ship and then tied it to a mast. The body of the dirigible could swing in the breeze, and yet passengers could safely get on and off the dirigible by walking down a gangplank to an open observation platform.\n",
    "The architects and engineers of the Empire State Building consulted with experts, taking tours of the equipment and mooring operations at the U.S. Naval Air Station in Lakehurst, New Jersey. The navy was the leader in the research and development of dirigibles in the United States. The navy even offered its dirigible, the Los Angeles, to be used in testing the mast. The architects also met with the president of a recently formed airship transport company that planned to offer dirigible service across the Pacific Ocean.\n",
    "When asked about the mooring mast, Al Smith commented:\n",
    "[It’s] on the level, all right. No kidding. We’re working on the thing now. One set of engineers here in New York is trying to dope out a practical, workable arrangement and the Government people in Washington are figuring on some safe way of mooring airships to this mast.\n",
    "Designing the Mast\n",
    "The architects could not simply drop a mooring mast on top of the Empire State Building’s flat roof. A thousand-foot dirigible moored at the top of the building, held by a single cable tether, would add stress to the building’s frame. The stress of the dirigible’s load and the wind pressure would have to be transmitted all the way to the building’s foundation, which was nearly eleven hundred feet below. The steel frame of the Empire State Building would have to be modified and strengthened to accommodate this new situation. Over sixty thousand dollars’ worth of modifications had to be made to the building’s framework.\n",
    "Rather than building a utilitarian mast without any ornamentation, the architects designed a shiny glass and chrome-nickel stainless steel tower that would be illuminated from inside, with a stepped-back design that imitated the overall shape of the building itself. The rocket-shaped mast would have four wings at its corners, of shiny aluminum, and would rise to a conical roof that would house the mooring arm. The winches and control machinery for the dirigible mooring would be housed in the base of the shaft itself, which also housed elevators and stairs to bring passengers down to the eighty-sixth floor, where baggage and ticket areas would be located.\n",
    "The building would now be 102 floors, with a glassed-in observation area on the 101st floor and an open observation platform on the 102nd floor. This observation area was to double as the boarding area for dirigible passengers.\n",
    "Once the architects had designed the mooring mast and made changes to the existing plans for the building’s skeleton, construction proceeded as planned. When the building had been framed to the 85th floor, the roof had to be completed before the framing for the mooring mast could take place. The mast also had a skeleton of steel and was clad in stainless steel with glass windows. Two months after the workers celebrated framing the entire building, they were back to raise an American flag again—this time at the top of the frame for the mooring mast.\n",
    "The Fate of the Mast\n",
    "The mooring mast of the Empire State Building was destined to never fulfill its purpose, for reasons that should have been apparent before it was ever constructed. The greatest reason was one of safety: Most dirigibles from outside of the United States used hydrogen rather than helium, and hydrogen is highly flammable. When the German dirigible Hindenburg was destroyed by fire in Lakehurst, New Jersey, on May 6, 1937, the owners of the Empire State Building realized how much worse that accident could have been if it had taken place above a densely populated area such as downtown New York.\n",
    "The greatest obstacle to the successful use of the mooring mast was nature itself. The winds on top of the building were constantly shifting due to violent air currents. Even if the dirigible were tethered to the mooring mast, the back of the ship would swivel around and around the mooring mast. Dirigibles moored in open landing fields could be weighted down in the back with lead weights, but using these at the Empire State Building, where they would be dangling high above pedestrians on the street, was neither practical nor safe.\n",
    "The other practical reason why dirigibles could not moor at the Empire State Building was an existing law against airships flying too low over urban areas. This law would make it illegal for a ship to ever tie up to the building or even approach the area, although two dirigibles did attempt to reach the building before the entire idea was dropped. In December 1930, the U.S. Navy dirigible Los Angeles approached the mooring mast but could not get close enough to tie up because of forceful winds. Fearing that the wind would blow the dirigible onto the sharp spires of other buildings in the area, which would puncture the dirigible’s shell, the captain could not even take his hands off the control levers. \n",
    "Two weeks later, another dirigible, the Goodyear blimp Columbia, attempted a publicity stunt where it would tie up and deliver a bundle of newspapers to the Empire State Building. Because the complete dirigible mooring equipment had never been installed, a worker atop the mooring mast would have to catch the bundle of papers on a rope dangling from the blimp. The papers were delivered in this fashion, but after this stunt the idea of using the mooring mast was shelved. In February 1931, Irving Clavan of the building’s architectural office said, “The as yet unsolved problems of mooring air ships to a fixed mast at such a height made it desirable to postpone to a later date the final installation of the landing gear.”\n",
    "By the late 1930s, the idea of using the mooring mast for dirigibles and their passengers had quietly disappeared. Dirigibles, instead of becoming the transportation of the future, had given way to airplanes. The rooms in the Empire State Building that had been set aside for the ticketing and baggage of dirigible passengers were made over into the world’s highest soda fountain and tea garden for use by the sightseers who flocked to the observation decks. The highest open observation deck, intended for disembarking passengers, has never been open to the public.\n",
    "\"\"\".replace(\"\\n\", \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8c3e07e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' When the Empire State Building was conceived, it was planned as the world’s tallest building, taller even than the new Chrysler Building that was being constructed at Forty-second Street and Lexington Avenue in New York. At seventy-seven stories, it was the tallest building before the Empire State began construction, and Al Smith was determined to outstrip it in height. The architect building the Chrysler Building, however, had a trick up his sleeve. He secretly constructed a 185-foot spire inside the building, and then shocked the public and the media by hoisting it up to the top of the Chrysler Building, bringing it to a height of 1,046 feet, 46 feet taller than the originally announced height of the Empire State Building. Al Smith realized that he was close to losing the title of world’s tallest building, and on December 11, 1929, he announced that the Empire State would now reach the height of 1,250 feet. He would add a top or a hat to the building that would be even more distinctive than any other building in the city. John Tauranac describes the plan: [The top of the Empire State Building] would be more than ornamental, more than a spire or dome or a pyramid put there to add a desired few feet to the height of the building or to mask something as mundane as a water tank. Their top, they said, would serve a higher calling. The Empire State Building would be equipped for an age of transportation that was then only the dream of aviation pioneers. This dream of the aviation pioneers was travel by dirigible, or zeppelin, and the Empire State Building was going to have a mooring mast at its top for docking these new airships, which would accommodate passengers on already existing transatlantic routes and new routes that were yet to come. The Age of Dirigibles By the 1920s, dirigibles were being hailed as the transportation of the future. Also known today as blimps, dirigibles were actually enormous steel-framed balloons, with envelopes of cotton fabric filled with hydrogen and helium to make them lighter than air. Unlike a balloon, a dirigible could be maneuvered by the use of propellers and rudders, and passengers could ride in the gondola, or enclosed compartment, under the balloon. Dirigibles had a top speed of eighty miles per hour, and they could cruise at seventy miles per hour for thousands of miles without needing refueling. Some were as long as one thousand feet, the same length as four blocks in New York City. The one obstacle to their expanded use in New York City was the lack of a suitable landing area. Al Smith saw an opportunity for his Empire State Building: A mooring mast added to the top of the building would allow dirigibles to anchor there for several hours for refueling or service, and to let passengers off and on. Dirigibles were docked by means of an electric winch, which hauled in a line from the front of the ship and then tied it to a mast. The body of the dirigible could swing in the breeze, and yet passengers could safely get on and off the dirigible by walking down a gangplank to an open observation platform. The architects and engineers of the Empire State Building consulted with experts, taking tours of the equipment and mooring operations at the U.S. Naval Air Station in Lakehurst, New Jersey. The navy was the leader in the research and development of dirigibles in the United States. The navy even offered its dirigible, the Los Angeles, to be used in testing the mast. The architects also met with the president of a recently formed airship transport company that planned to offer dirigible service across the Pacific Ocean. When asked about the mooring mast, Al Smith commented: [It’s] on the level, all right. No kidding. We’re working on the thing now. One set of engineers here in New York is trying to dope out a practical, workable arrangement and the Government people in Washington are figuring on some safe way of mooring airships to this mast. Designing the Mast The architects could not simply drop a mooring mast on top of the Empire State Building’s flat roof. A thousand-foot dirigible moored at the top of the building, held by a single cable tether, would add stress to the building’s frame. The stress of the dirigible’s load and the wind pressure would have to be transmitted all the way to the building’s foundation, which was nearly eleven hundred feet below. The steel frame of the Empire State Building would have to be modified and strengthened to accommodate this new situation. Over sixty thousand dollars’ worth of modifications had to be made to the building’s framework. Rather than building a utilitarian mast without any ornamentation, the architects designed a shiny glass and chrome-nickel stainless steel tower that would be illuminated from inside, with a stepped-back design that imitated the overall shape of the building itself. The rocket-shaped mast would have four wings at its corners, of shiny aluminum, and would rise to a conical roof that would house the mooring arm. The winches and control machinery for the dirigible mooring would be housed in the base of the shaft itself, which also housed elevators and stairs to bring passengers down to the eighty-sixth floor, where baggage and ticket areas would be located. The building would now be 102 floors, with a glassed-in observation area on the 101st floor and an open observation platform on the 102nd floor. This observation area was to double as the boarding area for dirigible passengers. Once the architects had designed the mooring mast and made changes to the existing plans for the building’s skeleton, construction proceeded as planned. When the building had been framed to the 85th floor, the roof had to be completed before the framing for the mooring mast could take place. The mast also had a skeleton of steel and was clad in stainless steel with glass windows. Two months after the workers celebrated framing the entire building, they were back to raise an American flag again—this time at the top of the frame for the mooring mast. The Fate of the Mast The mooring mast of the Empire State Building was destined to never fulfill its purpose, for reasons that should have been apparent before it was ever constructed. The greatest reason was one of safety: Most dirigibles from outside of the United States used hydrogen rather than helium, and hydrogen is highly flammable. When the German dirigible Hindenburg was destroyed by fire in Lakehurst, New Jersey, on May 6, 1937, the owners of the Empire State Building realized how much worse that accident could have been if it had taken place above a densely populated area such as downtown New York. The greatest obstacle to the successful use of the mooring mast was nature itself. The winds on top of the building were constantly shifting due to violent air currents. Even if the dirigible were tethered to the mooring mast, the back of the ship would swivel around and around the mooring mast. Dirigibles moored in open landing fields could be weighted down in the back with lead weights, but using these at the Empire State Building, where they would be dangling high above pedestrians on the street, was neither practical nor safe. The other practical reason why dirigibles could not moor at the Empire State Building was an existing law against airships flying too low over urban areas. This law would make it illegal for a ship to ever tie up to the building or even approach the area, although two dirigibles did attempt to reach the building before the entire idea was dropped. In December 1930, the U.S. Navy dirigible Los Angeles approached the mooring mast but could not get close enough to tie up because of forceful winds. Fearing that the wind would blow the dirigible onto the sharp spires of other buildings in the area, which would puncture the dirigible’s shell, the captain could not even take his hands off the control levers.  Two weeks later, another dirigible, the Goodyear blimp Columbia, attempted a publicity stunt where it would tie up and deliver a bundle of newspapers to the Empire State Building. Because the complete dirigible mooring equipment had never been installed, a worker atop the mooring mast would have to catch the bundle of papers on a rope dangling from the blimp. The papers were delivered in this fashion, but after this stunt the idea of using the mooring mast was shelved. In February 1931, Irving Clavan of the building’s architectural office said, “The as yet unsolved problems of mooring air ships to a fixed mast at such a height made it desirable to postpone to a later date the final installation of the landing gear.” By the late 1930s, the idea of using the mooring mast for dirigibles and their passengers had quietly disappeared. Dirigibles, instead of becoming the transportation of the future, had given way to airplanes. The rooms in the Empire State Building that had been set aside for the ticketing and baggage of dirigible passengers were made over into the world’s highest soda fountain and tea garden for use by the sightseers who flocked to the observation decks. The highest open observation deck, intended for disembarking passengers, has never been open to the public. '"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c394ee43",
   "metadata": {},
   "outputs": [],
   "source": [
    "dalle_list = set()\n",
    "with open(\"./dalle_chall.txt\", \"r\") as fp:\n",
    "    for line in fp:\n",
    "        for word in word_tokenize(line.strip()):\n",
    "            dalle_list.add(word.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c4ba8828",
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tune_results = client.fine_tuning.jobs.retrieve(fine_tuning_job.id)\n",
    "ft_model = fine_tune_results.fine_tuned_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7f34d7a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b241427ac854570a6d180062a429d4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "QUEUEING TASKS | :   0%|          | 0/1800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4c863a5995c4774bae567f7dd77c07f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "PROCESSING TASKS | :   0%|          | 0/1800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_87710/300172361.py:88: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  wvi = np.log(word_count) / np.log(\n",
      "/tmp/ipykernel_87710/300172361.py:88: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  wvi = np.log(word_count) / np.log(\n",
      "/tmp/ipykernel_87710/300172361.py:88: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  wvi = np.log(word_count) / np.log(\n",
      "/tmp/ipykernel_87710/300172361.py:88: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  wvi = np.log(word_count) / np.log(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7eb8dbb8f1fd494d95acffb9d5e0cfa5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "COLLECTING RESULTS | :   0%|          | 0/1800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "text = texts[1]\n",
    "\n",
    "\n",
    "def get_features(text: str, score: int):\n",
    "    stopwords_set = set(stopwords.words(\"english\"))\n",
    "    text_blob = TextBlob(text)\n",
    "    prompt = text\n",
    "\n",
    "    # Best effort correcting text\n",
    "    corrected_text = str(text_blob.correct())\n",
    "    text = re.sub(r\"[^a-zA-Z\\s.,\\']\", \" \", text)\n",
    "    corrected_text = re.sub(r\"[^a-zA-Z\\s.,\\']\", \" \", corrected_text)\n",
    "    tokens = word_tokenize(corrected_text)\n",
    "    tokens_prev = word_tokenize(text)\n",
    "\n",
    "    # Estimate errors\n",
    "    num_errors = sum(1 for w1, w2 in zip(tokens, tokens_prev) if w1 != w2)\n",
    "\n",
    "    sentences = [word_tokenize(sent) for sent in sent_tokenize(text)]\n",
    "    text_blob = TextBlob(corrected_text)\n",
    "\n",
    "    features = {\"num_errors\": num_errors}\n",
    "\n",
    "    # Surface features\n",
    "    num_characters = len(text)\n",
    "    words = [word for word in tokens if len(word) > 1]\n",
    "    word_count = len(words)\n",
    "    average_word_length = sum(len(word) for word in words) / len(words)\n",
    "    num_sentences = len(sentences)\n",
    "    average_sentence_length = sum(len(sent) for sent in sentences) / len(sentences)\n",
    "    num_different_words = len(set(words))\n",
    "    num_of_stopwords = len([word for word in words if word in stopwords_set])\n",
    "\n",
    "    features.update(\n",
    "        {\n",
    "            \"num_characters\": num_characters,\n",
    "            \"word_count\": word_count,\n",
    "            \"average_word_length\": average_word_length,\n",
    "            \"num_sentences\": num_sentences,\n",
    "            \"average_sentence_length\": average_sentence_length,\n",
    "            \"num_different_words\": num_different_words,\n",
    "            \"num_of_stopwords\": num_of_stopwords,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    syllable_estimates = [syllables.estimate(word) for word in words]\n",
    "    syllable_count = sum(syllable_estimates)\n",
    "\n",
    "    # Readability\n",
    "    flesch_reading_ease = (\n",
    "        206.835\n",
    "        - 1.015 * (word_count / num_sentences)\n",
    "        - 84.6 * (syllable_count / word_count)\n",
    "    )\n",
    "    flesch_kincaid_grade_level = (\n",
    "        0.39 * (word_count / num_sentences)\n",
    "        + 11.8 * (syllable_count / word_count)\n",
    "        - 15.59\n",
    "    )\n",
    "\n",
    "    difficult_word_count = len([word for word in words if word not in dalle_list])\n",
    "    dalle_chall_readability = 0.1579 * (\n",
    "        difficult_word_count / word_count * 100\n",
    "    ) + 0.0496 * (word_count / num_sentences)\n",
    "    # Automated readability index\n",
    "    ari = (\n",
    "        4.71 * (num_characters / word_count)\n",
    "        + 0.5 * (word_count / num_sentences)\n",
    "        - 21.43\n",
    "    )\n",
    "\n",
    "    group_t = sentences[0:10]\n",
    "    group_m = sentences[len(sentences) // 2 - 5 : len(sentences) // 2 + 5]\n",
    "    group_b = sentences[-10:]\n",
    "    nsw = 0\n",
    "    for sent in group_t + group_m + group_b:\n",
    "        for word in sent:\n",
    "            if syllables.estimate(word) >= 3:\n",
    "                nsw += 1\n",
    "\n",
    "    smog = 1.043 * np.sqrt(nsw) * 30 / len(sentences) + 3.1291\n",
    "\n",
    "    # LIX\n",
    "    B = len([w for w in tokens if w[0].isupper() or len(w) == 1])\n",
    "    C = len([w for w in words if len(w) > 6])\n",
    "    lix = word_count / B + (C * 100) / word_count\n",
    "\n",
    "    wvi = np.log(word_count) / np.log(\n",
    "        2 - np.log(num_different_words) / np.log(word_count)\n",
    "    )\n",
    "    gunning_fog_index = 0.4 * ((word_count / num_sentences) + 100 * (nsw / word_count))\n",
    "    \n",
    "\n",
    "    features.update(\n",
    "        {\n",
    "            \"flesch_reading_ease\": flesch_reading_ease,\n",
    "            \"flesch_kincaid_grade_level\": flesch_kincaid_grade_level,\n",
    "            \"dalle_chall_readability\": dalle_chall_readability,\n",
    "            \"ari\": ari,\n",
    "            \"smog\": smog,\n",
    "            \"lix\": lix,\n",
    "            \"wvi\": wvi,\n",
    "            \"gunning_fog_index\": gunning_fog_index,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Lexical diversity\n",
    "    ttr = num_different_words / word_count\n",
    "    rs, word_counts = defaultdict(int), defaultdict(int)\n",
    "    for word in words:\n",
    "        word_counts[word] += 1\n",
    "    for _, r in word_counts.items():\n",
    "        rs[r] += 1\n",
    "\n",
    "    yule_k = 1e4 * (sum(r**2 * vr for r, vr in rs.items()) - word_count) / word_count**2\n",
    "\n",
    "    min_range, max_range, trials = 35, 50, 5\n",
    "    ns = np.arange(min_range, max_range + 1)\n",
    "    ttrs = []\n",
    "    for idx, sample_size in enumerate(ns):\n",
    "        ttr = 0\n",
    "        if sample_size <= len(words):\n",
    "            for trial in range(trials):\n",
    "                word_list = np.random.choice(words, sample_size, replace=False)\n",
    "                ttr += len(set(word_list)) / len(word_list)\n",
    "            ttrs.append(ttr / trials)\n",
    "    ttrs = np.array(ttrs)\n",
    "    A = np.vstack([2 * (1 - ttrs) / ns[0:len(ttrs)]]).T\n",
    "    y = ttrs**2\n",
    "    d = np.linalg.lstsq(A, y, rcond=None)[0]\n",
    "    d_estimate = d[0]\n",
    "    hapax_legomena = rs[1]\n",
    "\n",
    "    guiraud = num_different_words / np.sqrt(word_count)\n",
    "    advanced_guiraud = difficult_word_count / np.sqrt(word_count)\n",
    "    features.update(\n",
    "        {\n",
    "            \"ttr\": ttr,\n",
    "            \"yule_k\": yule_k,\n",
    "            \"d_estimate\": d_estimate,\n",
    "            \"hapax_legomena\": rs[1],\n",
    "            \"guiraud\": guiraud,\n",
    "            \"advanced_guiraud\": advanced_guiraud,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # POS tags\n",
    "    pos_tags = defaultdict(int)\n",
    "    total_pos_tags = 0\n",
    "    unique_pos_tags = 0\n",
    "    for _, tag in text_blob.tags:\n",
    "        if tag not in pos_tags:\n",
    "            unique_pos_tags += 1\n",
    "        pos_tags[tag] += 1\n",
    "        total_pos_tags += 1\n",
    "\n",
    "    pos_dist = {}\n",
    "    for tag, count in pos_tags.items():\n",
    "        pos_dist[tag] = count/total_pos_tags\n",
    "        \n",
    "    features[\"total_pos_tags\"] = total_pos_tags\n",
    "    features[\"unique_pos_tags\"] = unique_pos_tags\n",
    "    features.update(pos_dist)\n",
    "    features.update(pos_tags)\n",
    "    \n",
    "    # Discourse patterns\n",
    "    doc = nlp(corrected_text)\n",
    "    egrid = EntityGrid(doc)\n",
    "    (\n",
    "        local_coherence_PU,\n",
    "        local_coherence_PW,\n",
    "        local_coherence_PACC,\n",
    "        local_coherence_PU_dist,\n",
    "        local_coherence_PW_dist,\n",
    "        local_coherence_PACC_dist,\n",
    "    ) = get_local_coherence(egrid)\n",
    "    \n",
    "    features.update({\n",
    "        \"ss_transitions\": egrid.get_ss_transitions(),\n",
    "        \"so_transitions\": egrid.get_so_transitions(),\n",
    "        \"sx_transitions\": egrid.get_sx_transitions(),\n",
    "        \"sn_transitions\": egrid.get_sn_transitions(),\n",
    "        \"os_transitions\": egrid.get_os_transitions(),\n",
    "        \"oo_transitions\": egrid.get_oo_transitions(),\n",
    "        \"ox_transitions\": egrid.get_ox_transitions(),\n",
    "        \"on_transitions\": egrid.get_on_transitions(),\n",
    "        \"xs_transitions\": egrid.get_xs_transitions(),\n",
    "        \"xo_transitions\": egrid.get_xo_transitions(),\n",
    "        \"xx_transitions\": egrid.get_xx_transitions(),\n",
    "        \"xn_transitions\": egrid.get_xn_transitions(),\n",
    "        \"ns_transitions\": egrid.get_ns_transitions(),\n",
    "        \"no_transitions\": egrid.get_no_transitions(),\n",
    "        \"nx_transitions\": egrid.get_nx_transitions(),\n",
    "        \"nn_transitions\": egrid.get_nn_transitions(),\n",
    "        \"local_coherence_PU\": local_coherence_PU,\n",
    "        \"local_coherence_PW\": local_coherence_PW,\n",
    "        \"local_coherence_PACC\": local_coherence_PACC,\n",
    "        \"local_coherence_PU_dist\": local_coherence_PU_dist,\n",
    "        \"local_coherence_PW_dist\": local_coherence_PW_dist,\n",
    "        \"local_coherence_PACC_dist\": local_coherence_PACC_dist,\n",
    "    })\n",
    "\n",
    "    # CVA Features\n",
    "    \n",
    "    # Compute weight scores for the essay\n",
    "    fi = vectorizer.transform([text])\n",
    "    max_f = fi.max()\n",
    "    wi = (fi / max_f).toarray() * np.log(n / ni)\n",
    "\n",
    "\n",
    "    # Maximum similarity to the best score category\n",
    "    max_sim_best = np.dot(wi, content_vectors[max_rating])/(norm(wi)*norm(content_vectors[max_rating]))\n",
    "    max_sim_best = max_sim_best[0]\n",
    "\n",
    "    pattern_cosine = 0\n",
    "    val_cos = 0\n",
    "    max_cos = 0\n",
    "    for i in range(min_rating, max_rating + 1):\n",
    "        # Similarity between content vectors of category i and the essay vector\n",
    "        cos = np.dot(wi, content_vectors[i])/(norm(wi)*norm(content_vectors[i]))\n",
    "        pattern_cosine += i * cos[0]\n",
    "        \n",
    "        # This is for the val cosine, e.g. cos_4 + cos_3 - cos_2 - cos_1\n",
    "        if i < (min(scores) + max(scores)) // 2:\n",
    "            val_cos -= cos[0]\n",
    "        else:\n",
    "            val_cos += cos[0]\n",
    "\n",
    "        # We are also looking for the score category closest to the essay\n",
    "        if cos[0] >= max_cos:\n",
    "            max_cos_val = i\n",
    "            max_cos = cos[0]\n",
    "            \n",
    "    fsource = vectorizer.transform([source_text])\n",
    "    max_fsource = fsource.max()\n",
    "    wsource = (fi / max_fsource).toarray() * np.log(n / ni)\n",
    "    cos_source = np.dot(wi.squeeze(), wsource.squeeze())/(norm(wi)*norm(wsource))\n",
    "    \n",
    "    features.update({\n",
    "        \"max_cos_val\": max_cos_val,\n",
    "        \"max_sim_best\": max_sim_best,\n",
    "        \"pattern_cosine\": pattern_cosine,\n",
    "        \"val_cos\": val_cos,\n",
    "        \"similarity_source_text\": cos_source,\n",
    "    })\n",
    "    \n",
    "    # GPT4 fine-tuned model as a feature\n",
    "    prompt = prompt + ' ->'\n",
    "    res = client.completions.create(\n",
    "        model=ft_model,\n",
    "        prompt=prompt,\n",
    "        max_tokens=2, temperature=0)\n",
    "\n",
    "    gpt_score = int(re.sub(r\"[^0-9]\", \"\", res.choices[0].text)) % 10\n",
    "    features[\"gpt_score\"] = gpt_score\n",
    "    \n",
    "# These are experimental, based on spatial measurements\n",
    "#     dist = cosine_distances(embeddings)\n",
    "#     nn_distances = np.min(dist + np.diag(np.diag(dist) + 10), axis=1)\n",
    "#     avg_nn_distance = np.mean(nn_distances)\n",
    "#     max_nn_distance = np.max(nn_distances)\n",
    "#     min_nn_distance = np.min(nn_distances)\n",
    "#     r_distance = 2*np.sqrt(dist.shape[0])*avg_nn_distance\n",
    "#     cum_freq_dist_nn_dist = np.mean(nn_distances <= avg_nn_distance)\n",
    "#     givenness = []\n",
    "#     for i in range(2, len(embeddings)):\n",
    "#         x = embeddings[0:i]\n",
    "#         u, s, vh = np.linalg.svd(x)\n",
    "#         orthonormal_vector = vh[-1]\n",
    "#         givenness.append(np.dot(embeddings[i], orthonormal_vector))\n",
    "\n",
    "#     avg_givenness = np.mean(givenness)\n",
    "#     max_givenness = np.max(givenness)\n",
    "#     min_givenness = np.min(givenness)\n",
    "#     givenness_proj = []\n",
    "#     for i in range(2, len(embeddings)):\n",
    "#         x = np.array(embeddings[0:i])\n",
    "#         A = x.T\n",
    "#         b = embeddings[i]\n",
    "#         c = np.linalg.lstsq(A, b, rcond=None)[0]\n",
    "#         bw = A.dot(c)\n",
    "#         bwo = b - bw\n",
    "#         N = np.dot(b, bwo)\n",
    "#         G = np.dot(b, bw)\n",
    "#         givenness_proj.append(N / (G + N))\n",
    "\n",
    "#     avg_givenness_proj = np.mean(givenness_proj)\n",
    "#     min_givenness_proj = np.min(givenness_proj)\n",
    "#     max_givenness_proj = np.max(givenness_proj)\n",
    "#     max_min_giv_ratio = max_givenness / min_givenness\n",
    "#     centroid = np.sum(embeddings, axis=0) / len(embeddings)\n",
    "#     dist_to_centroid = []\n",
    "#     for embedding in embeddings:\n",
    "#         dist_to_centroid.append(np.dot(centroid, embedding))\n",
    "\n",
    "#     avg_dist_to_centroid = np.mean(dist_to_centroid)\n",
    "#     max_dist_to_centroid = np.max(dist_to_centroid)\n",
    "#     min_dist_to_centroid = np.min(dist_to_centroid)\n",
    "#     std_distance = np.sqrt(np.sum(np.sum(\n",
    "#         (embeddings - centroid)**2, axis=1))/len(embeddings))\n",
    "#     relative_distance = std_distance / max_dist_to_centroid\n",
    "#     det_dist = np.linalg.det(dist)\n",
    "\n",
    "#     features.update({\n",
    "#         \"avg_nn_distance\": avg_nn_distance,\n",
    "#         \"max_nn_distance\": max_nn_distance,\n",
    "#         \"min_nn_distance\": min_nn_distance,\n",
    "#         \"r_distance\": r_distance,\n",
    "#         \"cum_freq_dist_nn_dist\": cum_freq_dist_nn_dist,\n",
    "#         \"avg_givenness\": avg_givenness,\n",
    "#         \"max_givenness\": max_givenness,\n",
    "#         \"min_givenness\": min_givenness,\n",
    "#         \"max_min_giv_ratio\": max_min_giv_ratio,\n",
    "#         \"avg_givenness_proj\": avg_givenness_proj,\n",
    "#         \"min_givenness_proj\": min_givenness_proj,\n",
    "#         \"max_givenness_proj\": max_givenness_proj,\n",
    "#         \"max_min_giv_ratio\": max_min_giv_ratio,\n",
    "#         \"avg_dist_to_centroid\": avg_dist_to_centroid,\n",
    "#         \"max_dist_to_centroid\": max_dist_to_centroid,\n",
    "#         \"min_dist_to_centroid\": min_dist_to_centroid,\n",
    "#         \"std_distance\": std_distance,\n",
    "#         \"relative_distance\": relative_distance,\n",
    "#         \"det_dist\": det_dist,\n",
    "#     })\n",
    "    features[\"score\"] = score\n",
    "\n",
    "    return features\n",
    "\n",
    "# print(get_features(texts[0], scores[1]))\n",
    "args = list(zip(texts, scores))\n",
    "train_result = pqdm(args, get_features, n_jobs=8, argument_type=\"args\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ca588a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_results = []\n",
    "for idx, res in enumerate(train_result):\n",
    "    if type(res) != dict:\n",
    "        continue\n",
    "    train_results.append(res)\n",
    "        \n",
    "essay_set_features = pd.DataFrame(train_results).fillna(0).replace([np.inf, -np.inf], np.nan).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3a395647",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = essay_set_features.drop([\"score\", \"gpt_score\"], axis=1), essay_set_features.score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "20f51ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestRegressor(\n",
    "    n_estimators=200, max_features=1/3, max_depth=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e685c0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def essay_metrics(clf, X, y_true):\n",
    "    y_pred = np.clip(np.round(clf.predict(X)), min_rating, max_rating)\n",
    "    qwk = quadratic_weighted_kappa(y_true, y_pred, min_rating=min_rating, max_rating=max_rating)\n",
    "    ea = np.sum(y_true == y_pred) / y_true.shape[0]\n",
    "    aa = np.sum((y_true - y_pred) <= 1) / y_true.shape[0]\n",
    "    return {\n",
    "        \"qwk\": qwk,\n",
    "        \"ea\": ea,\n",
    "        \"aa\": aa\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b4098e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert min_rating == y.min()\n",
    "assert max_rating == y.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f75e1f1",
   "metadata": {},
   "source": [
    "# DP Model Alone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ba4f5022",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "results = cross_validate(clf, X, y, scoring=essay_metrics, cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e2e29706",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8405458609989657, 0.7570621468926554, 1.0)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(results[\"test_qwk\"]), np.max(results[\"test_ea\"]), np.max(results[\"test_aa\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6576ed09",
   "metadata": {},
   "source": [
    "# GPT Model Alone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4331dce8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7810732366443675, 0.6841511562323745, 0.9966159052453468)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = essay_set_features.gpt_score\n",
    "y_true = essay_set_features.score\n",
    "qwk = quadratic_weighted_kappa(y_true, y_pred, min_rating=min_rating, max_rating=max_rating)\n",
    "ea = np.sum(y_true == y_pred) / y_true.shape[0]\n",
    "aa = np.sum((y_true - y_pred) <= 1) / y_true.shape[0]\n",
    "qwk, ea, aa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d582514",
   "metadata": {},
   "source": [
    "# Hybrid Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5bb33e5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8330096009767467, 0.7288135593220338, 1.0)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = essay_set_features.drop([\"score\"], axis=1), essay_set_features.score\n",
    "clf = RandomForestRegressor(\n",
    "    n_estimators=200, max_features=1/3, max_depth=12)\n",
    "results = cross_validate(clf, X, y, scoring=essay_metrics, cv=10)\n",
    "np.max(results[\"test_qwk\"]), np.max(results[\"test_ea\"]), np.max(results[\"test_aa\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2dc26f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
