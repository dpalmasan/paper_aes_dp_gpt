{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f104690",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import syllables\n",
    "from collections import defaultdict\n",
    "from pqdm.processes import pqdm\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from textblob import TextBlob\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import spacy\n",
    "from numpy.linalg import norm\n",
    "from tqdm import tqdm\n",
    "\n",
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "\n",
    "client = OpenAI(api_key=os.environ[\"OPEN_AI_API_KEY\"])\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "ESSAY_SET = 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415ccb28",
   "metadata": {},
   "source": [
    "# Define QWK Metric functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "782592ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix(rater_a, rater_b, min_rating=None, max_rating=None):\n",
    "    \"\"\"\n",
    "    Returns the confusion matrix between rater's ratings\n",
    "    \"\"\"\n",
    "    assert(len(rater_a) == len(rater_b))\n",
    "    if min_rating is None:\n",
    "        min_rating = min(rater_a + rater_b)\n",
    "    if max_rating is None:\n",
    "        max_rating = max(rater_a + rater_b)\n",
    "    num_ratings = int(max_rating - min_rating + 1)\n",
    "    conf_mat = [[0 for i in range(num_ratings)]\n",
    "                for j in range(num_ratings)]\n",
    "    for a, b in zip(rater_a, rater_b):\n",
    "        conf_mat[a - min_rating][b - min_rating] += 1\n",
    "    return conf_mat\n",
    "\n",
    "\n",
    "def histogram(ratings, min_rating=None, max_rating=None):\n",
    "    \"\"\"\n",
    "    Returns the counts of each type of rating that a rater made\n",
    "    \"\"\"\n",
    "    if min_rating is None:\n",
    "        min_rating = min(ratings)\n",
    "    if max_rating is None:\n",
    "        max_rating = max(ratings)\n",
    "    num_ratings = int(max_rating - min_rating + 1)\n",
    "    hist_ratings = [0 for x in range(num_ratings)]\n",
    "    for r in ratings:\n",
    "        hist_ratings[r - min_rating] += 1\n",
    "    return hist_ratings\n",
    "\n",
    "\n",
    "def quadratic_weighted_kappa(rater_a, rater_b, min_rating=None, max_rating=None):\n",
    "    \"\"\"\n",
    "    Calculates the quadratic weighted kappa\n",
    "    quadratic_weighted_kappa calculates the quadratic weighted kappa\n",
    "    value, which is a measure of inter-rater agreement between two raters\n",
    "    that provide discrete numeric ratings.  Potential values range from -1\n",
    "    (representing complete disagreement) to 1 (representing complete\n",
    "    agreement).  A kappa value of 0 is expected if all agreement is due to\n",
    "    chance.\n",
    "\n",
    "    quadratic_weighted_kappa(rater_a, rater_b), where rater_a and rater_b\n",
    "    each correspond to a list of integer ratings.  These lists must have the\n",
    "    same length.\n",
    "\n",
    "    The ratings should be integers, and it is assumed that they contain\n",
    "    the complete range of possible ratings.\n",
    "\n",
    "    quadratic_weighted_kappa(X, min_rating, max_rating), where min_rating\n",
    "    is the minimum possible rating, and max_rating is the maximum possible\n",
    "    rating\n",
    "    \"\"\"\n",
    "    rater_a = np.array(rater_a, dtype=int)\n",
    "    rater_b = np.array(rater_b, dtype=int)\n",
    "    assert(len(rater_a) == len(rater_b))\n",
    "    if min_rating is None:\n",
    "        min_rating = min(min(rater_a), min(rater_b))\n",
    "    if max_rating is None:\n",
    "        max_rating = max(max(rater_a), max(rater_b))\n",
    "    conf_mat = confusion_matrix(rater_a, rater_b,\n",
    "                                min_rating, max_rating)\n",
    "    num_ratings = len(conf_mat)\n",
    "    num_scored_items = float(len(rater_a))\n",
    "\n",
    "    hist_rater_a = histogram(rater_a, min_rating, max_rating)\n",
    "    hist_rater_b = histogram(rater_b, min_rating, max_rating)\n",
    "\n",
    "    numerator = 0.0\n",
    "    denominator = 0.0\n",
    "\n",
    "    for i in range(num_ratings):\n",
    "        for j in range(num_ratings):\n",
    "            expected_count = (hist_rater_a[i] * hist_rater_b[j]\n",
    "                              / num_scored_items)\n",
    "            d = pow(i - j, 2.0) / pow(num_ratings - 1, 2.0)\n",
    "            numerator += d * conf_mat[i][j] / num_scored_items\n",
    "            denominator += d * expected_count / num_scored_items\n",
    "\n",
    "    return 1.0 - numerator / denominator\n",
    "\n",
    "\n",
    "def linear_weighted_kappa(rater_a, rater_b, min_rating=None, max_rating=None):\n",
    "    \"\"\"\n",
    "    Calculates the linear weighted kappa\n",
    "    linear_weighted_kappa calculates the linear weighted kappa\n",
    "    value, which is a measure of inter-rater agreement between two raters\n",
    "    that provide discrete numeric ratings.  Potential values range from -1\n",
    "    (representing complete disagreement) to 1 (representing complete\n",
    "    agreement).  A kappa value of 0 is expected if all agreement is due to\n",
    "    chance.\n",
    "\n",
    "    linear_weighted_kappa(rater_a, rater_b), where rater_a and rater_b\n",
    "    each correspond to a list of integer ratings.  These lists must have the\n",
    "    same length.\n",
    "\n",
    "    The ratings should be integers, and it is assumed that they contain\n",
    "    the complete range of possible ratings.\n",
    "\n",
    "    linear_weighted_kappa(X, min_rating, max_rating), where min_rating\n",
    "    is the minimum possible rating, and max_rating is the maximum possible\n",
    "    rating\n",
    "    \"\"\"\n",
    "    assert(len(rater_a) == len(rater_b))\n",
    "    if min_rating is None:\n",
    "        min_rating = min(rater_a + rater_b)\n",
    "    if max_rating is None:\n",
    "        max_rating = max(rater_a + rater_b)\n",
    "    conf_mat = confusion_matrix(rater_a, rater_b,\n",
    "                                min_rating, max_rating)\n",
    "    num_ratings = len(conf_mat)\n",
    "    num_scored_items = float(len(rater_a))\n",
    "\n",
    "    hist_rater_a = histogram(rater_a, min_rating, max_rating)\n",
    "    hist_rater_b = histogram(rater_b, min_rating, max_rating)\n",
    "\n",
    "    numerator = 0.0\n",
    "    denominator = 0.0\n",
    "\n",
    "    for i in range(num_ratings):\n",
    "        for j in range(num_ratings):\n",
    "            expected_count = (hist_rater_a[i] * hist_rater_b[j]\n",
    "                              / num_scored_items)\n",
    "            d = abs(i - j) / float(num_ratings - 1)\n",
    "            numerator += d * conf_mat[i][j] / num_scored_items\n",
    "            denominator += d * expected_count / num_scored_items\n",
    "\n",
    "    return 1.0 - numerator / denominator\n",
    "\n",
    "\n",
    "def kappa(rater_a, rater_b, min_rating=None, max_rating=None):\n",
    "    \"\"\"\n",
    "    Calculates the kappa\n",
    "    kappa calculates the kappa\n",
    "    value, which is a measure of inter-rater agreement between two raters\n",
    "    that provide discrete numeric ratings.  Potential values range from -1\n",
    "    (representing complete disagreement) to 1 (representing complete\n",
    "    agreement).  A kappa value of 0 is expected if all agreement is due to\n",
    "    chance.\n",
    "\n",
    "    kappa(rater_a, rater_b), where rater_a and rater_b\n",
    "    each correspond to a list of integer ratings.  These lists must have the\n",
    "    same length.\n",
    "\n",
    "    The ratings should be integers, and it is assumed that they contain\n",
    "    the complete range of possible ratings.\n",
    "\n",
    "    kappa(X, min_rating, max_rating), where min_rating\n",
    "    is the minimum possible rating, and max_rating is the maximum possible\n",
    "    rating\n",
    "    \"\"\"\n",
    "    assert(len(rater_a) == len(rater_b))\n",
    "    if min_rating is None:\n",
    "        min_rating = min(rater_a + rater_b)\n",
    "    if max_rating is None:\n",
    "        max_rating = max(rater_a + rater_b)\n",
    "    conf_mat = confusion_matrix(rater_a, rater_b,\n",
    "                                min_rating, max_rating)\n",
    "    num_ratings = len(conf_mat)\n",
    "    num_scored_items = float(len(rater_a))\n",
    "\n",
    "    hist_rater_a = histogram(rater_a, min_rating, max_rating)\n",
    "    hist_rater_b = histogram(rater_b, min_rating, max_rating)\n",
    "\n",
    "    numerator = 0.0\n",
    "    denominator = 0.0\n",
    "\n",
    "    for i in range(num_ratings):\n",
    "        for j in range(num_ratings):\n",
    "            expected_count = (hist_rater_a[i] * hist_rater_b[j]\n",
    "                              / num_scored_items)\n",
    "            if i == j:\n",
    "                d = 0.0\n",
    "            else:\n",
    "                d = 1.0\n",
    "            numerator += d * conf_mat[i][j] / num_scored_items\n",
    "            denominator += d * expected_count / num_scored_items\n",
    "\n",
    "    return 1.0 - numerator / denominator\n",
    "\n",
    "\n",
    "def mean_quadratic_weighted_kappa(kappas, weights=None):\n",
    "    \"\"\"\n",
    "    Calculates the mean of the quadratic\n",
    "    weighted kappas after applying Fisher's r-to-z transform, which is\n",
    "    approximately a variance-stabilizing transformation.  This\n",
    "    transformation is undefined if one of the kappas is 1.0, so all kappa\n",
    "    values are capped in the range (-0.999, 0.999).  The reverse\n",
    "    transformation is then applied before returning the result.\n",
    "\n",
    "    mean_quadratic_weighted_kappa(kappas), where kappas is a vector of\n",
    "    kappa values\n",
    "\n",
    "    mean_quadratic_weighted_kappa(kappas, weights), where weights is a vector\n",
    "    of weights that is the same size as kappas.  Weights are applied in the\n",
    "    z-space\n",
    "    \"\"\"\n",
    "    kappas = np.array(kappas, dtype=float)\n",
    "    if weights is None:\n",
    "        weights = np.ones(np.shape(kappas))\n",
    "    else:\n",
    "        weights = weights / np.mean(weights)\n",
    "\n",
    "    # ensure that kappas are in the range [-.999, .999]\n",
    "    kappas = np.array([min(x, .999) for x in kappas])\n",
    "    kappas = np.array([max(x, -.999) for x in kappas])\n",
    "\n",
    "    z = 0.5 * np.log((1 + kappas) / (1 - kappas)) * weights\n",
    "    z = np.mean(z)\n",
    "    return (np.exp(2 * z) - 1) / (np.exp(2 * z) + 1)\n",
    "\n",
    "\n",
    "def weighted_mean_quadratic_weighted_kappa(solution, submission):\n",
    "    predicted_score = submission[submission.columns[-1]].copy()\n",
    "    predicted_score.name = \"predicted_score\"\n",
    "    if predicted_score.index[0] == 0:\n",
    "        predicted_score = predicted_score[:len(solution)]\n",
    "        predicted_score.index = solution.index\n",
    "    combined = solution.join(predicted_score, how=\"left\")\n",
    "    groups = combined.groupby(by=\"essay_set\")\n",
    "    kappas = [quadratic_weighted_kappa(group[1][\"essay_score\"], group[1][\"predicted_score\"]) for group in groups]\n",
    "    weights = [group[1][\"essay_weight\"].irow(0) for group in groups]\n",
    "    return mean_quadratic_weighted_kappa(kappas, weights=weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d4bafaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "UNIVERSAL_NOUN_TAGS = set([u\"NOUN\", u\"PRON\", u\"PROPN\"])\n",
    "\n",
    "ordered_transitions = [\n",
    "    u\"SS\",\n",
    "    u\"SO\",\n",
    "    u\"SX\",\n",
    "    u\"S-\",\n",
    "    u\"OS\",\n",
    "    u\"OO\",\n",
    "    u\"OX\",\n",
    "    u\"O-\",\n",
    "    u\"XS\",\n",
    "    u\"XO\",\n",
    "    u\"XX\",\n",
    "    u\"X-\",\n",
    "    u\"-S\",\n",
    "    u\"-O\",\n",
    "    u\"-X\",\n",
    "    u\"--\",\n",
    "]\n",
    "\n",
    "\n",
    "def dependency_mapping(dep: str) -> str:\n",
    "    \"\"\"Map dependency tag to entity grid tag.\n",
    "\n",
    "    We consider the notation provided in :cite:`barzilay2008modeling`:\n",
    "\n",
    "    +-----------+-----------------------------------+\n",
    "    | EGrid Tag | Dependency Tag                    |\n",
    "    +===========+===================================+\n",
    "    | S         | nsub, csubj, csubjpass, dsubjpass |\n",
    "    +-----------+-----------------------------------+\n",
    "    | O         | iobj, obj, pobj, dobj             |\n",
    "    +-----------+-----------------------------------+\n",
    "    | X         | For any other dependency tag      |\n",
    "    +-----------+-----------------------------------+\n",
    "\n",
    "    :param dep: Dependency tag\n",
    "    :type dep: string\n",
    "    :return: EGrid tag\n",
    "    :rtype: string\n",
    "    \"\"\"\n",
    "    if dep in {u\"nsubj\", u\"csubj\", u\"csubjpass\", u\"dsubjpass\"}:\n",
    "        return u\"S\"\n",
    "    if dep in {u\"iobj\", u\"obj\", u\"pobj\", u\"dobj\"}:\n",
    "        return u\"O\"\n",
    "\n",
    "    return \"X\"\n",
    "\n",
    "\n",
    "class EntityGrid(object):\n",
    "    \"\"\"Entity grid class.\n",
    "\n",
    "    Class Entity Grid, creates an entity grid from a doc, which is output of\n",
    "    applying spacy.nlp(text) to a text. Thus, this class depends on spacy\n",
    "    module. It only supports 2-transitions entity grid.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, doc):\n",
    "        \"\"\"Construct EntityGrid object.\"\"\"\n",
    "        # Initialization\n",
    "        entity_map = dict()\n",
    "        entity_grid = dict()\n",
    "        i = 1\n",
    "        entity_map[\"s%d\" % i] = []\n",
    "        entity_features = {\n",
    "            u\"SS\": 0,\n",
    "            u\"SO\": 0,\n",
    "            u\"SX\": 0,\n",
    "            u\"S-\": 0,\n",
    "            u\"OS\": 0,\n",
    "            u\"OO\": 0,\n",
    "            u\"OX\": 0,\n",
    "            u\"O-\": 0,\n",
    "            u\"XS\": 0,\n",
    "            u\"XO\": 0,\n",
    "            u\"XX\": 0,\n",
    "            u\"X-\": 0,\n",
    "            u\"-S\": 0,\n",
    "            u\"-O\": 0,\n",
    "            u\"-X\": 0,\n",
    "            u\"--\": 0,\n",
    "        }\n",
    "\n",
    "\n",
    "        n_sent = len(list(doc.sents))\n",
    "\n",
    "        # To get coherence measurements we need at least 2 sentences\n",
    "        if n_sent < 2:\n",
    "            raise RuntimeError(\n",
    "                \"Entity grid needs at least two sentences, found: {}\".format(\n",
    "                    n_sent\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # For each sentence, get dependencies and its grammatical role\n",
    "        for sent in doc.sents:\n",
    "            for token in sent:\n",
    "                if token.pos_ in UNIVERSAL_NOUN_TAGS:\n",
    "                    entity_map[\"s%d\" % i].append(\n",
    "                        (token.text.upper(), token.dep_)\n",
    "                    )\n",
    "                    if token.text.upper() not in entity_grid:\n",
    "                        entity_grid[token.text.upper()] = [u\"-\"] * n_sent\n",
    "            i += 1\n",
    "            entity_map[\"s%d\" % i] = []\n",
    "\n",
    "        # Last iteration will create an extra element, so I remove it.\n",
    "        entity_map.pop(\"s%d\" % i)\n",
    "\n",
    "        # Fill entity grid\n",
    "        for i in range(n_sent):\n",
    "            sentence = \"s%d\" % (i + 1)\n",
    "            for entity, dep in entity_map[sentence]:\n",
    "                if entity_grid[entity][i] == u\"-\":\n",
    "                    entity_grid[entity][i] = dependency_mapping(dep)\n",
    "                elif dependency_mapping(dep) == u\"S\":\n",
    "                    entity_grid[entity][i] = dependency_mapping(dep)\n",
    "                elif (\n",
    "                    dependency_mapping(dep) == u\"O\"\n",
    "                    and entity_grid[entity][i] == u\"X\"\n",
    "                ):\n",
    "                    entity_grid[entity][i] = dependency_mapping(dep)\n",
    "\n",
    "        # Compute feature vector, we consider transitions of length 2\n",
    "        total_transitions = (n_sent - 1) * len(entity_grid.keys())\n",
    "\n",
    "        for entity in entity_grid:\n",
    "            for i in range(n_sent - 1):\n",
    "                # Transition type found (e.g. S-)\n",
    "                transition = (\n",
    "                    entity_grid[entity][i] + entity_grid[entity][i + 1]\n",
    "                )\n",
    "\n",
    "                # Adding 1 to transition count\n",
    "                entity_features[transition] += 1\n",
    "\n",
    "        for prob in entity_features:\n",
    "            if total_transitions != 0:\n",
    "                entity_features[prob] /= float(total_transitions)\n",
    "            else:\n",
    "                entity_features[prob] = 0.0\n",
    "\n",
    "        self.__grid = entity_grid\n",
    "        self.__n_sent = n_sent\n",
    "        self.__prob = entity_features\n",
    "\n",
    "    def get_ss_transitions(self) -> float:\n",
    "        \"\"\"Get SS transitions.\n",
    "\n",
    "        :return: Ratio of transitions\n",
    "        :rtype: float\n",
    "        \"\"\"\n",
    "        return self.__prob[u\"SS\"]\n",
    "\n",
    "    def get_so_transitions(self) -> float:\n",
    "        \"\"\"Get SO transitions.\n",
    "\n",
    "        :return: Ratio of transitions\n",
    "        :rtype: float\n",
    "        \"\"\"\n",
    "        return self.__prob[u\"SO\"]\n",
    "\n",
    "    def get_sx_transitions(self) -> float:\n",
    "        \"\"\"Get SX transitions.\n",
    "\n",
    "        :return: Ratio of transitions\n",
    "        :rtype: float\n",
    "        \"\"\"\n",
    "        return self.__prob[u\"SX\"]\n",
    "\n",
    "    def get_sn_transitions(self) -> float:\n",
    "        \"\"\"Get S- transitions.\n",
    "\n",
    "        :return: Ratio of transitions\n",
    "        :rtype: float\n",
    "        \"\"\"\n",
    "        return self.__prob[u\"S-\"]\n",
    "\n",
    "    def get_os_transitions(self) -> float:\n",
    "        \"\"\"Get OS transitions.\n",
    "\n",
    "        :return: Ratio of transitions\n",
    "        :rtype: float\n",
    "        \"\"\"\n",
    "        return self.__prob[u\"OS\"]\n",
    "\n",
    "    def get_oo_transitions(self) -> float:\n",
    "        \"\"\"Get OO transitions.\n",
    "\n",
    "        :return: Ratio of transitions\n",
    "        :rtype: float\n",
    "        \"\"\"\n",
    "        return self.__prob[u\"OO\"]\n",
    "\n",
    "    def get_ox_transitions(self) -> float:\n",
    "        \"\"\"Get OX transitions.\n",
    "\n",
    "        :return: Ratio of transitions\n",
    "        :rtype: float\n",
    "        \"\"\"\n",
    "        return self.__prob[u\"OX\"]\n",
    "\n",
    "    def get_on_transitions(self) -> float:\n",
    "        \"\"\"Get O- transitions.\n",
    "\n",
    "        :return: Ratio of transitions\n",
    "        :rtype: float\n",
    "        \"\"\"\n",
    "        return self.__prob[u\"O-\"]\n",
    "\n",
    "    def get_xs_transitions(self) -> float:\n",
    "        \"\"\"Get XS transitions.\n",
    "\n",
    "        :return: Ratio of transitions\n",
    "        :rtype: float\n",
    "        \"\"\"\n",
    "        return self.__prob[u\"XS\"]\n",
    "\n",
    "    def get_xo_transitions(self) -> float:\n",
    "        \"\"\"Get XO transitions.\n",
    "\n",
    "        :return: Ratio of transitions\n",
    "        :rtype: float\n",
    "        \"\"\"\n",
    "        return self.__prob[u\"XO\"]\n",
    "\n",
    "    def get_xx_transitions(self) -> float:\n",
    "        \"\"\"Get XX transitions.\n",
    "\n",
    "        :return: Ratio of transitions\n",
    "        :rtype: float\n",
    "        \"\"\"\n",
    "        return self.__prob[u\"XX\"]\n",
    "\n",
    "    def get_xn_transitions(self) -> float:\n",
    "        \"\"\"Get X- transitions.\n",
    "\n",
    "        :return: Ratio of transitions\n",
    "        :rtype: float\n",
    "        \"\"\"\n",
    "        return self.__prob[u\"X-\"]\n",
    "\n",
    "    def get_ns_transitions(self) -> float:\n",
    "        \"\"\"Get -S transitions.\n",
    "\n",
    "        :return: Ratio of transitions\n",
    "        :rtype: float\n",
    "        \"\"\"\n",
    "        return self.__prob[u\"-S\"]\n",
    "\n",
    "    def get_no_transitions(self) -> float:\n",
    "        \"\"\"Get -O transitions.\n",
    "\n",
    "        :return: Ratio of transitions\n",
    "        :rtype: float\n",
    "        \"\"\"\n",
    "        return self.__prob[u\"-O\"]\n",
    "\n",
    "    def get_nx_transitions(self) -> float:\n",
    "        \"\"\"Get -X transitions.\n",
    "\n",
    "        :return: Ratio of transitions\n",
    "        :rtype: float\n",
    "        \"\"\"\n",
    "        return self.__prob[u\"-X\"]\n",
    "\n",
    "    def get_nn_transitions(self) -> float:\n",
    "        \"\"\"Get -- transitions.\n",
    "\n",
    "        :return: Ratio of transitions\n",
    "        :rtype: float\n",
    "        \"\"\"\n",
    "        return self.__prob[u\"--\"]\n",
    "\n",
    "    def get_egrid(self) -> dict:\n",
    "        \"\"\"Return obtained entity grid (for debugging purposes).\n",
    "\n",
    "        :return: entity grid represented as a dict\n",
    "        :rtype: dict\n",
    "        \"\"\"\n",
    "        return self.__grid\n",
    "\n",
    "    def get_sentence_count(self) -> int:\n",
    "        \"\"\"Return sentence count obtained while processing.\n",
    "\n",
    "        :return: Number of sentences\n",
    "        :rtype: int\n",
    "        \"\"\"\n",
    "        return self.__n_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10224389",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighting_syntactic_role(entity_role: str) -> int:\n",
    "    \"\"\"Return weight given an entity grammatical role.\n",
    "\n",
    "    Weighting scheme for syntactic role of an entity. This uses the heuristic\n",
    "    from :cite:`guinaudeau2013graph`, which is:\n",
    "\n",
    "    +-----------+--------+\n",
    "    | EGrid Tag | Weight |\n",
    "    +===========+========+\n",
    "    | S         | 3      |\n",
    "    +-----------+--------+\n",
    "    | O         | 2      |\n",
    "    +-----------+--------+\n",
    "    | X         | 1      |\n",
    "    +-----------+--------+\n",
    "    | dash      | 0      |\n",
    "    +-----------+--------+\n",
    "\n",
    "    :param entity_role: Entity grammatical role (S, O, X, -)\n",
    "    :type entity_role: string\n",
    "    :return: Role weight\n",
    "    :rtype: int\n",
    "    \"\"\"\n",
    "    if entity_role == u\"S\":\n",
    "        return 3\n",
    "    elif entity_role == u\"O\":\n",
    "        return 2\n",
    "    elif entity_role == u\"X\":\n",
    "        return 1\n",
    "\n",
    "    return 0\n",
    "\n",
    "\n",
    "def get_local_coherence(egrid: EntityGrid) -> [float, float, float, float]:\n",
    "    \"\"\"Get local coherence from entity grid.\n",
    "\n",
    "    This method gets the coherence value using all the approaches described\n",
    "    in :cite:`guinaudeau2013graph`. This include:\n",
    "\n",
    "    * local_coherence_PU\n",
    "    * local_coherence_PW\n",
    "    * local_coherence_PACC\n",
    "    * local_coherence_PU_dist\n",
    "    * local_coherence_PW_dist\n",
    "    * local_coherence_PACC_dist\n",
    "\n",
    "    :param egrid: An EntityGrid object.\n",
    "    :type egrid: EntityGrid\n",
    "    :return: Local coherence based on different heuristics\n",
    "    :rtype: tuple of floats\n",
    "    \"\"\"\n",
    "    n_sent = egrid.get_sentence_count()\n",
    "\n",
    "    # If entity grid is not valid\n",
    "    if n_sent < 2:\n",
    "        return (0.0, 0.0, 0.0, 0.0, 0.0, 0.0)\n",
    "\n",
    "    PW = [[0] * n_sent for i in range(n_sent)]\n",
    "\n",
    "    # Weight Matrix for PACC, syntactic information is accounted for by\n",
    "    # integrating the edges of the bipartite graph\n",
    "    W = [[0] * n_sent for i in range(n_sent)]\n",
    "\n",
    "    grid = egrid.get_egrid()\n",
    "    for entity in grid:\n",
    "        for i in range(n_sent):\n",
    "            for j in range(i + 1, n_sent):\n",
    "                if grid[entity][i] != u\"-\" and grid[entity][j] != u\"-\":\n",
    "                    PW[i][j] += 1\n",
    "                    W[i][j] += weighting_syntactic_role(\n",
    "                        grid[entity][i]\n",
    "                    ) * weighting_syntactic_role(grid[entity][j])\n",
    "\n",
    "    PU = [list(map(lambda x: x != 0, PWi)) for PWi in PW]\n",
    "\n",
    "    local_coherence_PU = 0.0\n",
    "    local_coherence_PW = 0.0\n",
    "    local_coherence_PACC = 0.0\n",
    "    for i in range(n_sent):\n",
    "        local_coherence_PW += sum(PW[i])\n",
    "        local_coherence_PU += sum(PU[i])\n",
    "        local_coherence_PACC += sum(W[i])\n",
    "\n",
    "    local_coherence_PW /= n_sent\n",
    "    local_coherence_PU /= n_sent\n",
    "    local_coherence_PACC /= n_sent\n",
    "\n",
    "    # Weighting projection graphs\n",
    "    PU_weighted = list(PU)\n",
    "    PW_weighted = list(PW)\n",
    "    PACC_weighted = list(W)\n",
    "    for i in range(n_sent):\n",
    "        for j in range(i + 1, n_sent):\n",
    "            PU_weighted[i][j] = PU[i][j] / float(j - i)\n",
    "            PW_weighted[i][j] = PW[i][j] / float(j - i)\n",
    "            PACC_weighted[i][j] = W[i][j] / float(j - i)\n",
    "\n",
    "    local_coherence_PU_dist = 0.0\n",
    "    local_coherence_PW_dist = 0.0\n",
    "    local_coherence_PACC_dist = 0.0\n",
    "    for i in range(n_sent):\n",
    "        local_coherence_PW_dist += sum(PW_weighted[i])\n",
    "        local_coherence_PU_dist += sum(PU_weighted[i])\n",
    "        local_coherence_PACC_dist += sum(PACC_weighted[i])\n",
    "\n",
    "    local_coherence_PW_dist /= n_sent\n",
    "    local_coherence_PU_dist /= n_sent\n",
    "    local_coherence_PACC_dist /= n_sent\n",
    "    return (\n",
    "        local_coherence_PU,\n",
    "        local_coherence_PW,\n",
    "        local_coherence_PACC,\n",
    "        local_coherence_PU_dist,\n",
    "        local_coherence_PW_dist,\n",
    "        local_coherence_PACC_dist,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bed78faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\n",
    "    \"./training_set_rel3.tsv\",\n",
    "    sep=\"\\t\",\n",
    "    encoding=\"ISO-8859-1\",\n",
    "    usecols=[\"essay_id\", \"essay_set\", \"essay\", \"domain1_score\", \"domain2_score\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af931dec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({15: 85,\n",
       "         13: 82,\n",
       "         17: 160,\n",
       "         23: 53,\n",
       "         16: 199,\n",
       "         18: 118,\n",
       "         12: 86,\n",
       "         10: 55,\n",
       "         19: 88,\n",
       "         14: 105,\n",
       "         21: 68,\n",
       "         24: 96,\n",
       "         9: 49,\n",
       "         6: 20,\n",
       "         11: 56,\n",
       "         20: 99,\n",
       "         7: 28,\n",
       "         8: 50,\n",
       "         22: 62,\n",
       "         4: 4,\n",
       "         5: 4,\n",
       "         2: 1,\n",
       "         3: 1})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "df = df[df.essay_set == ESSAY_SET]\n",
    "Counter(df.domain1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3875b716",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_rating, max_rating = int(df.domain1_score.min()), int(df.domain1_score.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b69e1cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ft = pd.DataFrame({\n",
    "    \"prompt\": df.essay.tolist(),\n",
    "    \"completion\": df.domain1_score.tolist()})\n",
    "\n",
    "df_ft.to_json(f\"essay_set{ESSAY_SET}.jsonl\", orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a7d1079",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>completion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Patience is when your waiting .I was patience ...</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I am not a patience person, like I cant sit i...</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>One day I was at basketball practice and I was...</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I going to write about a time when I went to t...</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>It can be very hard for somebody to be patient...</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              prompt  completion\n",
       "0  Patience is when your waiting .I was patience ...          15\n",
       "1  I am not a patience person, like I cant sit i...          13\n",
       "2  One day I was at basketball practice and I was...          15\n",
       "3  I going to write about a time when I went to t...          17\n",
       "4  It can be very hard for somebody to be patient...          13"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ft.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14aeec6e",
   "metadata": {},
   "source": [
    "# Generate data for Fine Tuning\n",
    "\n",
    "We run the following tool:\n",
    "\n",
    "`openai tools fine_tunes.prepare_data -f essay_set7.jsonl -q`\n",
    "\n",
    "This will generate two data splits for the fine-tuning. One for training and other for validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ddc38e9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FineTuningJob(id='ftjob-x5VQpZ3PTL8xXkA9VU0cCDgy', created_at=1721593806, error=Error(code=None, message=None, param=None), fine_tuned_model=None, finished_at=None, hyperparameters=Hyperparameters(n_epochs='auto', batch_size='auto', learning_rate_multiplier='auto'), model='babbage-002', object='fine_tuning.job', organization_id='org-JihYzTh2GjJjoPtZZ0kQdsbr', result_files=[], seed=1352712089, status='validating_files', trained_tokens=None, training_file='file-EIzGO2fphJuzRFl2KbN1lGUQ', validation_file='file-bvPyMoXnvI4bychC6MWHEkAT', estimated_finish=None, integrations=[], user_provided_suffix=None)\n"
     ]
    }
   ],
   "source": [
    "train_file = client.files.create(file=open(f\"essay_set{ESSAY_SET}_prepared_train.jsonl\", \"rb\"), purpose=\"fine-tune\")\n",
    "valid_file = client.files.create(file=open(f\"essay_set{ESSAY_SET}_prepared_valid.jsonl\", \"rb\"), purpose=\"fine-tune\")\n",
    "fine_tuning_job = client.fine_tuning.jobs.create(training_file=train_file.id, validation_file=valid_file.id, model=\"babbage-002\")\n",
    "print(fine_tuning_job)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8695dcf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts, scores = (\n",
    "    df.essay.tolist(),\n",
    "    df.domain1_score.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "52762763",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(min_df=10)\n",
    "\n",
    "doc_term = vectorizer.fit_transform(df.essay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d0d9b0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting CVA features https://files.eric.ed.gov/fulltext/ED525309.pdf\n",
    "\n",
    "content_vectors = {}\n",
    "n = df.shape[0]\n",
    "ni = doc_term.getnnz(axis=0)\n",
    "for i in range(min_rating, max_rating + 1):\n",
    "    essay_score_cat = df[df.domain1_score == i]\n",
    "    freqs = vectorizer.transform(essay_score_cat.essay)\n",
    "    \n",
    "    # Get frequencies score categories\n",
    "    fis = freqs.sum(axis=0)\n",
    "    fis = np.asarray(fis).reshape(-1)\n",
    "    try:\n",
    "        max_fs = freqs.max()\n",
    "    except:\n",
    "        print(i)\n",
    "        raise\n",
    "    content_vectors[i] = fis/max_fs * np.log(n / ni)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3b724582",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1263,)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content_vectors[3].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3287935e",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_text = \"\"\"\n",
    "Write about patience. Being patient means that you are understanding and tolerant. A patient person experience difficulties without complaining.\n",
    "Do only one of the following: write a story about a time when you were patient OR write a story about a time when someone you know was patient OR write a story in your own way about patience.\n",
    "\"\"\".replace(\"\\n\", \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8c3e07e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Write about patience. Being patient means that you are understanding and tolerant. A patient person experience difficulties without complaining. Do only one of the following: write a story about a time when you were patient OR write a story about a time when someone you know was patient OR write a story in your own way about patience. '"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c394ee43",
   "metadata": {},
   "outputs": [],
   "source": [
    "dalle_list = set()\n",
    "with open(\"./dalle_chall.txt\", \"r\") as fp:\n",
    "    for line in fp:\n",
    "        for word in word_tokenize(line.strip()):\n",
    "            dalle_list.add(word.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c4ba8828",
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tune_results = client.fine_tuning.jobs.retrieve(fine_tuning_job.id)\n",
    "ft_model = fine_tune_results.fine_tuned_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7f34d7a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ebeebf7ceb14da7a2ee26dc36b1a656",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "QUEUEING TASKS | :   0%|          | 0/1569 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c3b1a0c1ebc4787ac0b67ebd1185646",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "PROCESSING TASKS | :   0%|          | 0/1569 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_88187/4105047819.py:88: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  wvi = np.log(word_count) / np.log(\n",
      "/tmp/ipykernel_88187/4105047819.py:88: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  wvi = np.log(word_count) / np.log(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "872792af9151467d81ce29bb3efeda25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "COLLECTING RESULTS | :   0%|          | 0/1569 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "text = texts[1]\n",
    "\n",
    "\n",
    "def get_features(text: str, score: int):\n",
    "    stopwords_set = set(stopwords.words(\"english\"))\n",
    "    text_blob = TextBlob(text)\n",
    "    prompt = text\n",
    "\n",
    "    # Best effort correcting text\n",
    "    corrected_text = str(text_blob.correct())\n",
    "    text = re.sub(r\"[^a-zA-Z\\s.,\\']\", \" \", text)\n",
    "    corrected_text = re.sub(r\"[^a-zA-Z\\s.,\\']\", \" \", corrected_text)\n",
    "    tokens = word_tokenize(corrected_text)\n",
    "    tokens_prev = word_tokenize(text)\n",
    "\n",
    "    # Estimate errors\n",
    "    num_errors = sum(1 for w1, w2 in zip(tokens, tokens_prev) if w1 != w2)\n",
    "\n",
    "    sentences = [word_tokenize(sent) for sent in sent_tokenize(text)]\n",
    "    text_blob = TextBlob(corrected_text)\n",
    "\n",
    "    features = {\"num_errors\": num_errors}\n",
    "\n",
    "    # Surface features\n",
    "    num_characters = len(text)\n",
    "    words = [word for word in tokens if len(word) > 1]\n",
    "    word_count = len(words)\n",
    "    average_word_length = sum(len(word) for word in words) / len(words)\n",
    "    num_sentences = len(sentences)\n",
    "    average_sentence_length = sum(len(sent) for sent in sentences) / len(sentences)\n",
    "    num_different_words = len(set(words))\n",
    "    num_of_stopwords = len([word for word in words if word in stopwords_set])\n",
    "\n",
    "    features.update(\n",
    "        {\n",
    "            \"num_characters\": num_characters,\n",
    "            \"word_count\": word_count,\n",
    "            \"average_word_length\": average_word_length,\n",
    "            \"num_sentences\": num_sentences,\n",
    "            \"average_sentence_length\": average_sentence_length,\n",
    "            \"num_different_words\": num_different_words,\n",
    "            \"num_of_stopwords\": num_of_stopwords,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    syllable_estimates = [syllables.estimate(word) for word in words]\n",
    "    syllable_count = sum(syllable_estimates)\n",
    "\n",
    "    # Readability\n",
    "    flesch_reading_ease = (\n",
    "        206.835\n",
    "        - 1.015 * (word_count / num_sentences)\n",
    "        - 84.6 * (syllable_count / word_count)\n",
    "    )\n",
    "    flesch_kincaid_grade_level = (\n",
    "        0.39 * (word_count / num_sentences)\n",
    "        + 11.8 * (syllable_count / word_count)\n",
    "        - 15.59\n",
    "    )\n",
    "\n",
    "    difficult_word_count = len([word for word in words if word not in dalle_list])\n",
    "    dalle_chall_readability = 0.1579 * (\n",
    "        difficult_word_count / word_count * 100\n",
    "    ) + 0.0496 * (word_count / num_sentences)\n",
    "    # Automated readability index\n",
    "    ari = (\n",
    "        4.71 * (num_characters / word_count)\n",
    "        + 0.5 * (word_count / num_sentences)\n",
    "        - 21.43\n",
    "    )\n",
    "\n",
    "    group_t = sentences[0:10]\n",
    "    group_m = sentences[len(sentences) // 2 - 5 : len(sentences) // 2 + 5]\n",
    "    group_b = sentences[-10:]\n",
    "    nsw = 0\n",
    "    for sent in group_t + group_m + group_b:\n",
    "        for word in sent:\n",
    "            if syllables.estimate(word) >= 3:\n",
    "                nsw += 1\n",
    "\n",
    "    smog = 1.043 * np.sqrt(nsw) * 30 / len(sentences) + 3.1291\n",
    "\n",
    "    # LIX\n",
    "    B = len([w for w in tokens if w[0].isupper() or len(w) == 1])\n",
    "    C = len([w for w in words if len(w) > 6])\n",
    "    lix = word_count / B + (C * 100) / word_count\n",
    "\n",
    "    wvi = np.log(word_count) / np.log(\n",
    "        2 - np.log(num_different_words) / np.log(word_count)\n",
    "    )\n",
    "    gunning_fog_index = 0.4 * ((word_count / num_sentences) + 100 * (nsw / word_count))\n",
    "    \n",
    "\n",
    "    features.update(\n",
    "        {\n",
    "            \"flesch_reading_ease\": flesch_reading_ease,\n",
    "            \"flesch_kincaid_grade_level\": flesch_kincaid_grade_level,\n",
    "            \"dalle_chall_readability\": dalle_chall_readability,\n",
    "            \"ari\": ari,\n",
    "            \"smog\": smog,\n",
    "            \"lix\": lix,\n",
    "            \"wvi\": wvi,\n",
    "            \"gunning_fog_index\": gunning_fog_index,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Lexical diversity\n",
    "    ttr = num_different_words / word_count\n",
    "    rs, word_counts = defaultdict(int), defaultdict(int)\n",
    "    for word in words:\n",
    "        word_counts[word] += 1\n",
    "    for _, r in word_counts.items():\n",
    "        rs[r] += 1\n",
    "\n",
    "    yule_k = 1e4 * (sum(r**2 * vr for r, vr in rs.items()) - word_count) / word_count**2\n",
    "\n",
    "    min_range, max_range, trials = 35, 50, 5\n",
    "    ns = np.arange(min_range, max_range + 1)\n",
    "    ttrs = []\n",
    "    for idx, sample_size in enumerate(ns):\n",
    "        ttr = 0\n",
    "        if sample_size <= len(words):\n",
    "            for trial in range(trials):\n",
    "                word_list = np.random.choice(words, sample_size, replace=False)\n",
    "                ttr += len(set(word_list)) / len(word_list)\n",
    "            ttrs.append(ttr / trials)\n",
    "    ttrs = np.array(ttrs)\n",
    "    A = np.vstack([2 * (1 - ttrs) / ns[0:len(ttrs)]]).T\n",
    "    y = ttrs**2\n",
    "    d = np.linalg.lstsq(A, y, rcond=None)[0]\n",
    "    d_estimate = d[0]\n",
    "    hapax_legomena = rs[1]\n",
    "\n",
    "    guiraud = num_different_words / np.sqrt(word_count)\n",
    "    advanced_guiraud = difficult_word_count / np.sqrt(word_count)\n",
    "    features.update(\n",
    "        {\n",
    "            \"ttr\": ttr,\n",
    "            \"yule_k\": yule_k,\n",
    "            \"d_estimate\": d_estimate,\n",
    "            \"hapax_legomena\": rs[1],\n",
    "            \"guiraud\": guiraud,\n",
    "            \"advanced_guiraud\": advanced_guiraud,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # POS tags\n",
    "    pos_tags = defaultdict(int)\n",
    "    total_pos_tags = 0\n",
    "    unique_pos_tags = 0\n",
    "    for _, tag in text_blob.tags:\n",
    "        if tag not in pos_tags:\n",
    "            unique_pos_tags += 1\n",
    "        pos_tags[tag] += 1\n",
    "        total_pos_tags += 1\n",
    "\n",
    "    pos_dist = {}\n",
    "    for tag, count in pos_tags.items():\n",
    "        pos_dist[tag] = count/total_pos_tags\n",
    "        \n",
    "    features[\"total_pos_tags\"] = total_pos_tags\n",
    "    features[\"unique_pos_tags\"] = unique_pos_tags\n",
    "    features.update(pos_dist)\n",
    "    features.update(pos_tags)\n",
    "    \n",
    "    # Discourse patterns\n",
    "    doc = nlp(corrected_text)\n",
    "    egrid = EntityGrid(doc)\n",
    "    (\n",
    "        local_coherence_PU,\n",
    "        local_coherence_PW,\n",
    "        local_coherence_PACC,\n",
    "        local_coherence_PU_dist,\n",
    "        local_coherence_PW_dist,\n",
    "        local_coherence_PACC_dist,\n",
    "    ) = get_local_coherence(egrid)\n",
    "    \n",
    "    features.update({\n",
    "        \"ss_transitions\": egrid.get_ss_transitions(),\n",
    "        \"so_transitions\": egrid.get_so_transitions(),\n",
    "        \"sx_transitions\": egrid.get_sx_transitions(),\n",
    "        \"sn_transitions\": egrid.get_sn_transitions(),\n",
    "        \"os_transitions\": egrid.get_os_transitions(),\n",
    "        \"oo_transitions\": egrid.get_oo_transitions(),\n",
    "        \"ox_transitions\": egrid.get_ox_transitions(),\n",
    "        \"on_transitions\": egrid.get_on_transitions(),\n",
    "        \"xs_transitions\": egrid.get_xs_transitions(),\n",
    "        \"xo_transitions\": egrid.get_xo_transitions(),\n",
    "        \"xx_transitions\": egrid.get_xx_transitions(),\n",
    "        \"xn_transitions\": egrid.get_xn_transitions(),\n",
    "        \"ns_transitions\": egrid.get_ns_transitions(),\n",
    "        \"no_transitions\": egrid.get_no_transitions(),\n",
    "        \"nx_transitions\": egrid.get_nx_transitions(),\n",
    "        \"nn_transitions\": egrid.get_nn_transitions(),\n",
    "        \"local_coherence_PU\": local_coherence_PU,\n",
    "        \"local_coherence_PW\": local_coherence_PW,\n",
    "        \"local_coherence_PACC\": local_coherence_PACC,\n",
    "        \"local_coherence_PU_dist\": local_coherence_PU_dist,\n",
    "        \"local_coherence_PW_dist\": local_coherence_PW_dist,\n",
    "        \"local_coherence_PACC_dist\": local_coherence_PACC_dist,\n",
    "    })\n",
    "\n",
    "    # CVA Features\n",
    "    \n",
    "    # Compute weight scores for the essay\n",
    "    fi = vectorizer.transform([text])\n",
    "    max_f = fi.max()\n",
    "    wi = (fi / max_f).toarray() * np.log(n / ni)\n",
    "\n",
    "\n",
    "    # Maximum similarity to the best score category\n",
    "    max_sim_best = np.dot(wi, content_vectors[max_rating])/(norm(wi)*norm(content_vectors[max_rating]))\n",
    "    max_sim_best = max_sim_best[0]\n",
    "\n",
    "    pattern_cosine = 0\n",
    "    val_cos = 0\n",
    "    max_cos = 0\n",
    "    for i in range(min_rating, max_rating + 1):\n",
    "        # Similarity between content vectors of category i and the essay vector\n",
    "        cos = np.dot(wi, content_vectors[i])/(norm(wi)*norm(content_vectors[i]))\n",
    "        pattern_cosine += i * cos[0]\n",
    "        \n",
    "        # This is for the val cosine, e.g. cos_4 + cos_3 - cos_2 - cos_1\n",
    "        if i < (min(scores) + max(scores)) // 2:\n",
    "            val_cos -= cos[0]\n",
    "        else:\n",
    "            val_cos += cos[0]\n",
    "\n",
    "        # We are also looking for the score category closest to the essay\n",
    "        if cos[0] >= max_cos:\n",
    "            max_cos_val = i\n",
    "            max_cos = cos[0]\n",
    "            \n",
    "    fsource = vectorizer.transform([source_text])\n",
    "    max_fsource = fsource.max()\n",
    "    wsource = (fi / max_fsource).toarray() * np.log(n / ni)\n",
    "    cos_source = np.dot(wi.squeeze(), wsource.squeeze())/(norm(wi)*norm(wsource))\n",
    "    \n",
    "    features.update({\n",
    "        \"max_cos_val\": max_cos_val,\n",
    "        \"max_sim_best\": max_sim_best,\n",
    "        \"pattern_cosine\": pattern_cosine,\n",
    "        \"val_cos\": val_cos,\n",
    "        \"similarity_source_text\": cos_source,\n",
    "    })\n",
    "    \n",
    "    # GPT4 fine-tuned model as a feature\n",
    "    prompt = prompt + ' ->'\n",
    "    res = client.completions.create(\n",
    "        model=ft_model,\n",
    "        prompt=prompt,\n",
    "        max_tokens=2, temperature=0)\n",
    "\n",
    "    gpt_score = int(re.sub(r\"[^0-9]\", \"\", res.choices[0].text)) % 100\n",
    "    features[\"gpt_score\"] = gpt_score\n",
    "    \n",
    "# These are experimental, based on spatial measurements\n",
    "#     dist = cosine_distances(embeddings)\n",
    "#     nn_distances = np.min(dist + np.diag(np.diag(dist) + 10), axis=1)\n",
    "#     avg_nn_distance = np.mean(nn_distances)\n",
    "#     max_nn_distance = np.max(nn_distances)\n",
    "#     min_nn_distance = np.min(nn_distances)\n",
    "#     r_distance = 2*np.sqrt(dist.shape[0])*avg_nn_distance\n",
    "#     cum_freq_dist_nn_dist = np.mean(nn_distances <= avg_nn_distance)\n",
    "#     givenness = []\n",
    "#     for i in range(2, len(embeddings)):\n",
    "#         x = embeddings[0:i]\n",
    "#         u, s, vh = np.linalg.svd(x)\n",
    "#         orthonormal_vector = vh[-1]\n",
    "#         givenness.append(np.dot(embeddings[i], orthonormal_vector))\n",
    "\n",
    "#     avg_givenness = np.mean(givenness)\n",
    "#     max_givenness = np.max(givenness)\n",
    "#     min_givenness = np.min(givenness)\n",
    "#     givenness_proj = []\n",
    "#     for i in range(2, len(embeddings)):\n",
    "#         x = np.array(embeddings[0:i])\n",
    "#         A = x.T\n",
    "#         b = embeddings[i]\n",
    "#         c = np.linalg.lstsq(A, b, rcond=None)[0]\n",
    "#         bw = A.dot(c)\n",
    "#         bwo = b - bw\n",
    "#         N = np.dot(b, bwo)\n",
    "#         G = np.dot(b, bw)\n",
    "#         givenness_proj.append(N / (G + N))\n",
    "\n",
    "#     avg_givenness_proj = np.mean(givenness_proj)\n",
    "#     min_givenness_proj = np.min(givenness_proj)\n",
    "#     max_givenness_proj = np.max(givenness_proj)\n",
    "#     max_min_giv_ratio = max_givenness / min_givenness\n",
    "#     centroid = np.sum(embeddings, axis=0) / len(embeddings)\n",
    "#     dist_to_centroid = []\n",
    "#     for embedding in embeddings:\n",
    "#         dist_to_centroid.append(np.dot(centroid, embedding))\n",
    "\n",
    "#     avg_dist_to_centroid = np.mean(dist_to_centroid)\n",
    "#     max_dist_to_centroid = np.max(dist_to_centroid)\n",
    "#     min_dist_to_centroid = np.min(dist_to_centroid)\n",
    "#     std_distance = np.sqrt(np.sum(np.sum(\n",
    "#         (embeddings - centroid)**2, axis=1))/len(embeddings))\n",
    "#     relative_distance = std_distance / max_dist_to_centroid\n",
    "#     det_dist = np.linalg.det(dist)\n",
    "\n",
    "#     features.update({\n",
    "#         \"avg_nn_distance\": avg_nn_distance,\n",
    "#         \"max_nn_distance\": max_nn_distance,\n",
    "#         \"min_nn_distance\": min_nn_distance,\n",
    "#         \"r_distance\": r_distance,\n",
    "#         \"cum_freq_dist_nn_dist\": cum_freq_dist_nn_dist,\n",
    "#         \"avg_givenness\": avg_givenness,\n",
    "#         \"max_givenness\": max_givenness,\n",
    "#         \"min_givenness\": min_givenness,\n",
    "#         \"max_min_giv_ratio\": max_min_giv_ratio,\n",
    "#         \"avg_givenness_proj\": avg_givenness_proj,\n",
    "#         \"min_givenness_proj\": min_givenness_proj,\n",
    "#         \"max_givenness_proj\": max_givenness_proj,\n",
    "#         \"max_min_giv_ratio\": max_min_giv_ratio,\n",
    "#         \"avg_dist_to_centroid\": avg_dist_to_centroid,\n",
    "#         \"max_dist_to_centroid\": max_dist_to_centroid,\n",
    "#         \"min_dist_to_centroid\": min_dist_to_centroid,\n",
    "#         \"std_distance\": std_distance,\n",
    "#         \"relative_distance\": relative_distance,\n",
    "#         \"det_dist\": det_dist,\n",
    "#     })\n",
    "    features[\"score\"] = score\n",
    "\n",
    "    return features\n",
    "\n",
    "# print(get_features(texts[0], scores[0]))\n",
    "args = list(zip(texts, scores))\n",
    "train_result = pqdm(args, get_features, n_jobs=8, argument_type=\"args\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ca588a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_results = []\n",
    "for idx, res in enumerate(train_result):\n",
    "    if type(res) != dict:\n",
    "        continue\n",
    "    train_results.append(res)\n",
    "        \n",
    "essay_set_features = pd.DataFrame(train_results).fillna(0).replace([np.inf, -np.inf], np.nan).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3a395647",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = essay_set_features.drop([\"score\", \"gpt_score\"], axis=1), essay_set_features.score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "20f51ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestRegressor(\n",
    "    n_estimators=200, max_features=1/3, max_depth=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e685c0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def essay_metrics(clf, X, y_true):\n",
    "    y_pred = np.clip(np.round(clf.predict(X)), min_rating, max_rating)\n",
    "    qwk = quadratic_weighted_kappa(y_true, y_pred, min_rating=min_rating, max_rating=max_rating)\n",
    "    ea = np.sum(y_true == y_pred) / y_true.shape[0]\n",
    "    aa = np.sum((y_true - y_pred) <= 1) / y_true.shape[0]\n",
    "    return {\n",
    "        \"qwk\": qwk,\n",
    "        \"ea\": ea,\n",
    "        \"aa\": aa\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b4098e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert min_rating == y.min()\n",
    "assert max_rating == y.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f75e1f1",
   "metadata": {},
   "source": [
    "# DP Model Alone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ba4f5022",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "results = cross_validate(clf, X, y, scoring=essay_metrics, cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e2e29706",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8563738597142373, 0.2967741935483871, 0.8129032258064516)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(results[\"test_qwk\"]), np.max(results[\"test_ea\"]), np.max(results[\"test_aa\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6576ed09",
   "metadata": {},
   "source": [
    "# GPT Model Alone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4331dce8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8252885421562464, 0.22336991607488701, 0.7817947062621046)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = essay_set_features.gpt_score\n",
    "y_true = essay_set_features.score\n",
    "qwk = quadratic_weighted_kappa(y_true, y_pred, min_rating=min_rating, max_rating=max_rating)\n",
    "ea = np.sum(y_true == y_pred) / y_true.shape[0]\n",
    "aa = np.sum((y_true - y_pred) <= 1) / y_true.shape[0]\n",
    "qwk, ea, aa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d582514",
   "metadata": {},
   "source": [
    "# Hybrid Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5bb33e5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.870720083607377, 0.2645161290322581, 0.8193548387096774)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = essay_set_features.drop([\"score\"], axis=1), essay_set_features.score\n",
    "clf = RandomForestRegressor(\n",
    "    n_estimators=200, max_features=1/3, max_depth=12)\n",
    "results = cross_validate(clf, X, y, scoring=essay_metrics, cv=10)\n",
    "np.max(results[\"test_qwk\"]), np.max(results[\"test_ea\"]), np.max(results[\"test_aa\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2dc26f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
